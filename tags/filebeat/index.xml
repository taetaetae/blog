<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>filebeat - Tag - 👨‍💻꿈꾸는 태태태의 공간</title><link>https://taetaetae.github.io/tags/filebeat/</link><description>filebeat - Tag - 👨‍💻꿈꾸는 태태태의 공간</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 15 Feb 2021 17:50:12 +0900</lastBuildDate><atom:link href="https://taetaetae.github.io/tags/filebeat/" rel="self" type="application/rss+xml"/><item><title>Elastic Stack으로 코로나19 대시보드 만들기 - 1부 : 파이프라인 구성</title><link>https://taetaetae.github.io/posts/make-dashboards-from-elasticstack/</link><pubDate>Mon, 15 Feb 2021 17:50:12 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/make-dashboards-from-elasticstack/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/make-dashboards-from-elasticstack-1/logo.jpg" referrerpolicy="no-referrer">
            </div>﻿　얼마 전에 필자의 블로그를 보고 어느 교육 기관에서 ElasticStack에 대한 강의 요청이 들어왔다. 사실 관련 기술에 대해 지식이 아주 깊고 해박한 게 아니라서 약간의 반감부터 들었지만 ElasticStack을 전혀 모르는 사람들 기준으로 어떻게 돌아가는지에 대해서만 간단하게 소개하는 정도로 하면 된다고 하여 조심스럽지만 떨리는 마음으로 열심히 준비를 하기 시작했다. 그런데, 이런저런 이유로 갑자기 강의를 할 수 없게 되었고 그간 준비했던 내용들이 너무 아쉽지만 아무 소용이 없게 되어버렸다. 그냥 중단하기엔 아쉬운 마음이 너무 커서 준비했던 내용 중에 &lsquo;데이터를 가지고 대시보드를 만드는 부분&rsquo;은 누군가에겐 도움이 될까 싶어 블로그에 정리를 해보려 한다.
 ﻿강의를 준비한 올해 1월 중순엔 Elasticsearch 버전이 7.10.2이었는데 블로그를 쓰고 있는 지금은 벌써 7.11으로 버전 업 되었다. 내가 아는 오픈소스 중에 버전업이 가장 빠른데 그렇다고 기능이 확 바뀌거나 습득하기 어렵게 바뀌진 않았다. 그만큼 사용자가 무엇을 원하는지 명확히 알고 작은 단위로 조금씩 바뀌어 가는 모습이 꽤 인상적이다.
 　﻿작년 초부터 코로나19 바이러스가 전 세계적으로 퍼지기 시작했고 아직까지도 진행 중이다. 나도 전염되는 건 아닐까 하는 두려움에 어디에서 얼마나 발생했는지를 확인하기 어렵던 시절 우리나라의 뛰어난 개발자들은 누가 시키지도 않았는데 정말 감사하게도 그 현황을 한눈에 볼 수 있도록 여러 유형으로 코로나19 바이러스 대시보드를 만들기 시작한다. 그 덕분에 좀 더 현황을 보기에 편해졌고 더욱 조심하게 되는 계기가 되었다고 생각한다. 이제는 포털사이트나 각종 매체를 통해 손쉽게 코로나19 바이러스의 현황을 볼 수 있지만 이러한 데이터를 가지고 검색엔진이지만 대시보드를 구축하는데 훌륭한 기능을 가지고 있는 ElasticStack을 활용해서 &lsquo;나만의 대시보드&rsquo;를 만드는 걸 정리해보고자 한다. 본 포스팅의 일회성으로 데이터를 가지고 대시보드를 만드는 것에서 끝나는 게 아니라 지속적으로 데이터가 업데이트된다는 가정하에 전반적인 &ldquo;파이프라인&quot;을 구축한 뒤 대시보드를 만들어 두고 데이터만 갱신하면 자동으로 대시보드 또한 업데이트되는 것을 목적으로 한다. 전체 흐름" 전체 흐름 
 ﻿글을 모두 작성하고 보니 양이 생각보다 길어져서 데이터를 조회하고 필터링하여 Elasticsearch에 인덱싱 하는 대시보드를 만들기 위한 일종의 &ldquo;데이터 파이프라인&quot;을 구성하는 부분과 만들어진 데이터 기반으로 Kibana의 다양한 기능을 활용하여 대시보드를 만드는 2개의 포스팅으로 나누어 정리해보겠다.
 ﻿　최종적으로 만들게 될 대시보드의 모습은 다음과 같다. 최종 목표!" 최종 목표! 
대시보드 구성 준비 　﻿예전에는 Elasticsearch, Logstash, Kibana 3가지를 가지고 ELK라 불리다 Beat라는 경량 수집기 제품이 등장하며 이 모든 걸 ElasticStack라 부르기 시작했다. (공식 홈페이지 참고) 먼저 어떤 목표와 어떤 순서로 대시보드를 구성할 것인지에 대해 정리해봐야겠다.﻿
데이터 　﻿데이터는 공공데이터 포털에서 가져오려다 조회를 해보니 누락되는 날짜도 있었고 원하는 데이터의 품질이 생각보다 좋지 않아서 다른 곳을 찾아봐야 했다. 그러다 간결하게 정리한 데이터가 깃헙에 공개가 되어 있어서 그것을 사용하려 한다. 해당 데이터는 https://coronaboard.kr/ 에서도 사용되는 데이터라고 한다. ﻿
데이터 전처리(preprocessing) 　﻿원하는 데이터는 위 깃헙에서 제공하는 데이터 중에 지역별 발생 현황. 해당 데이터를 살펴보면 요일별로 데이터가 &lsquo;누적&rsquo;되어 저장되어 있다. 즉, 서울지역 기준으로 2020년 2월 17일에 14명이 발생했고 2020년 2월 18일에 한 명도 발생하지 않았는데 14명으로 &lsquo;누적&rsquo;되어 저장되어 있다. 사실 이대로 해도 큰 문제는 없지만 어디까지나 별도의 가공 없이 최대한 원본 데이터(raw) 가 있어야 데이터 분석 시 다양하게 활용이 가능하기에 데이터를 분석하기 전에 전처리 과정이 필요했다. 정리하면, 집계 수가 누적되지 않고 날짜 기준으로 집계된 수만 있는 데이터를 원했다.﻿
　﻿필자는 주로 java를 가지고 개발을 하지만 가끔 간단한 스크립트성 개발은 python을 활용하는 편이기에 다소 이쁜 코드는 아니지만 데이터를 조작하려 아래와 같은 코드를 작성하였다.
import csv, requests import pandas as pd CSV_URL = &#39;https://raw.githubusercontent.com/jooeungen/coronaboard_kr/master/kr_regional_daily.csv&#39; # 확진, 사망, 격리해제 yesterday_data = {} yesterday_data[&#39;서울&#39;] = [0, 0, 0] yesterday_data[&#39;부산&#39;] = [0, 0, 0] yesterday_data[&#39;대구&#39;] = [0, 0, 0] yesterday_data[&#39;인천&#39;] = [0, 0, 0] yesterday_data[&#39;광주&#39;] = [0, 0, 0] yesterday_data[&#39;대전&#39;] = [0, 0, 0] yesterday_data[&#39;울산&#39;] = [0, 0, 0] yesterday_data[&#39;세종&#39;] = [0, 0, 0] yesterday_data[&#39;경기&#39;] = [0, 0, 0] yesterday_data[&#39;강원&#39;] = [0, 0, 0] yesterday_data[&#39;충북&#39;] = [0, 0, 0] yesterday_data[&#39;충남&#39;] = [0, 0, 0] yesterday_data[&#39;전북&#39;] = [0, 0, 0] yesterday_data[&#39;전남&#39;] = [0, 0, 0] yesterday_data[&#39;경북&#39;] = [0, 0, 0] yesterday_data[&#39;경남&#39;] = [0, 0, 0] yesterday_data[&#39;제주&#39;] = [0, 0, 0] yesterday_data[&#39;검역&#39;] = [0, 0, 0] flag = False csv_data = [] with requests.]]></description></item><item><title>누구나 할 수 있는 엑세스 로그 분석 따라 해보기 (by Elastic Stack)</title><link>https://taetaetae.github.io/2019/02/10/access-log-to-elastic-stack/</link><pubDate>Sun, 10 Feb 2019 14:37:31 +0000</pubDate><author>Author</author><guid>https://taetaetae.github.io/2019/02/10/access-log-to-elastic-stack/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/access-log-to-elastic-stack/elastic_stack.jpg" referrerpolicy="no-referrer">
            </div>필자가 Elastic Stack을 알게된건 2017년 어느 여름 동기형이 공부하고 있는것을 보고 호기심에 따라하며 시작하게 되었다. 그때까지만 해도 버전이 2.x 였는데 지금 글을 쓰고있는 2019년 2월초 최신버전이 6.6이니 정말 빠르게 변화하는것 같다. 빠르게 변화하는 버전만큼 사람들의 관심도 (드라마틱하게는 아니지만) 꾸준히 늘어나 개인적으로, 그리고 실무에서도 활용하는 범위가 많아지고 있는것 같다.
 trends.embed.renderExploreWidget("TIMESERIES", {"comparisonItem":[{"keyword":"elasticsearch","geo":"KR","time":"today 5-y"}],"category":0,"property":""}, {"exploreQuery":"date=today%205-y&geo=KR&q=elasticsearch","guestPath":"https://trends.google.co.kr:443/trends/embed/"});  그래서 그런지 최근들어 (아주 코딱지만큼 조금이라도 더 해본) 필자에게 Elastic Stack 사용방법에 대해 물어보는 주변 지인들이 늘어나고 있다. 그리고 예전에 한창 공부했을때의 버전보다 많이 바꼈기에 이 기회에 &ldquo;그대로 따라만 하면 Elastic Stack을 구성할 수 있을만한 글&quot;을 써보고자 한다. 사실 필자가 예전에 &ldquo;도큐먼트를 보기엔 너무 어려워 보이는 느낌적인 느낌&rdquo; 때문에 삽질하며 구성한 힘들었던 기억을 되살려 최대한 심플하고 처음 해보는 사람도 따라하기만 하면 &ldquo;아~ 이게 Elastic Stack 이구나!&rdquo;, &ldquo;이런식으로 돌아가는 거구나!&rdquo; 하는 도움을 주고 싶다.
 + 그러면서 최신버전도 살펴보고&hellip; 1석2조, 이런게 바로 블로그를 하는 이유이지 않을까? 다시한번 말하지만 도큐먼트가 최고 지침서이긴 하다&hellip;
 Elastic 공식 홈페이지에 가면 각 제품군들에 대해 그림으로 된 자세한 설명과 도큐먼트가 있지만 이들을 어떤식으로 조합하여 사용하는지에 대한 전체적인 흐름을 볼 수 있는 곳은 없어 보인다. (지금 보면 도큐먼트가 그 어디보다 설명이 잘되어 있다고 생각되지만 사전 지식이 전혀없는 상태에서는 봐도봐도 어려워 보였다.) 이번 포스팅에서는 Apache access log를 Elasticsearch에 인덱싱 하는 방법에 대해 설명해보고자 한다.
전체적인 흐름 필자는 글보다는 그림을 좋아하는 편이라 전체적인 흐름을 그림으로 먼저 보자.
  외부에서의 접근이 발생하면 apache 웹서버에서 설정한 경로에 access log가 파일로 생성이 되거나 있는 파일에 추가가 된다. 해당 파일에는 한줄당 하나의 엑세스 정보가 남게 된다. fileBeat에서 해당 파일을 트래킹 하고 있다가 라인이 추가되면 이 정보를 logstash 에게 전달해준다. logastsh 는 filebeat에서 전달한 정보를 특정 port로 input 받는다. 받은 정보를 filter 과정을 통해 각 정보를 분할 및 정제한다. (ip, uri, time 등) 정리된 정보를 elasticsearch 에 ouput 으로 보낸다. (정확히 말하면 인덱싱을 한다.) elasticsearch 에 인덱싱 된 정보를 키바나를 통해 손쉽게 분석을 한다.  한번의 설치고 일련의 과정이 뚝딱 된다면 너무 편하겠지만, 각각의 레이어가 나뉘어져있는 이유는 하는 역활이 전문적으로(?) 나뉘어져 있고 각 레이어에서는 세부 설정을 통해 보다 효율적으로 데이터를 관리할 수 있기 때문이다.
 beats라는 레이어가 나오기 전에는 logstash에서 직접 file을 바라보곤 했었는데 beats가 logstash 보다 가벼운 shipper 목적으로 나온 agent 이다보니 통상 logstash 앞단에 filebeat를 위치시키곤 한다고 한다.
 전체적인 그림은 위와 같고, 이제 이 글을 보고있는 여러분들이 따라할 차례이다. 각 레이어별로 하나씩 설치를 해보며 구성을 해보자. 설치순서는 데이터 흐름의 순서에 맞춰 다음과 같은 순서로 설치를 해야 효율적으로 볼수가 있다. (아래순서대로 하지 않을경우 설치/시작/종료 를 각각의 타이밍에 맞추어 해줘야 할것 같아 복잡할것같다.)
elasticsearch → logstash → kibana → filebeat 이 포스팅은 CentOS 7.4에서 Java 1.8, apache 2.2가 설치되어있다는 가정하에 보면 될듯하다. 또한 각 레이어별 설명은 구글링을 하거나 Elastic 공식 홈페이지에 가보면 자세히 나와있으니 기본 설명은 안하는것으로 하고, 각 레이어의 세부 설정은 하지 않는것으로 한다.
Elasticsearch 공식 홈페이지
다운받고 압축풀고 심볼릭 경로 만들고 (심볼릭 경로는 선택사항) $ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.0.tar.gz $ tar zxvf elasticsearch-6.6.0.tar.gz $ ln -s elasticsearch-6.6.0 elasticsearch 설정 파일을 열고 추가해준다. $ cd elasticsearch/conf $ vi elasticsearch.yml path.data: /~~~/data/elasticsearch (기본경로에서 변경할때추가) path.logs: /~~~/logs/elasticsearch network.host: 0.0.0.0 # 외부에서 접근이 가능하도록 (실제 ip를 적어줘도 됨) elasticsearch 의 시작과 종료를 조금이나마 편하게 하기위해 스크립트를 작성해줌 (이것또한 선택사항) $ cd ../bin $ echo &#39;.]]></description></item></channel></rss>