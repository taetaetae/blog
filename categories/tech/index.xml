<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Tech - Category - 👨‍💻꿈꾸는 태태태의 공간</title><link>https://taetaetae.github.io/categories/tech/</link><description>Tech - Category - 👨‍💻꿈꾸는 태태태의 공간</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 04 Apr 2021 18:54:00 +0900</lastBuildDate><atom:link href="https://taetaetae.github.io/categories/tech/" rel="self" type="application/rss+xml"/><item><title>공모주 알리미 개발 후기 - 3부</title><link>https://taetaetae.github.io/posts/public-offering-notice-3/</link><pubDate>Sun, 04 Apr 2021 18:54:00 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/public-offering-notice-3/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/public-offering-notice-3/logo.jpg" referrerpolicy="no-referrer">
            </div>﻿공모주 알리미라는 토이 프로젝트 개발기의 마지막 포스팅이다. 토이 프로젝트를 왜 시작하게 되었고 어떻게 설계하게 되었으며 데이터는 어떤 식으로 수집하고 그 데이터를 어떤 방법으로 사용자들에게 알림을 보내기까지 알아보았다. 이제는 이러한 일련의 &lsquo;파이프라인&rsquo;을 자동화해야 할 시간이다. 사람이 직접 수동으로 로컬 컴퓨터에서 위 파이프라인을 실행하는 것이 아니라 별도의 서버에 해당 애플리케이션이 등록되어 있고 이를 어떤 무언가에 의해 트리거링을 해주는 방식으로 말이다.﻿
 1부 : 프로젝트 설계, 데이터 수집 2부 : 수집한 데이터 알림 발송 3부 : 서버 선정 및 릴리즈  서버 선정 　﻿1부에서 이야기했던 것처럼 heroku라는 PaaS(Platform as a service)를 사용하면 될 것 같았다. 무료 플랜으로도 설계했던 서비스 내용을 모두 소화 가능했기 때문이다. 앞서 만든 Spring Boot Application 을 heroku에 배포를 해보자.
　heroku에서 새로운 &lsquo;App&rsquo;을 생성한다. 아래에서 보여주고 있는 화면대로 App name을 지정하고 만들기만 하면 끝. 그러면 배포 방법이 여러 가지가 나오는데 heroku에서 제공하는 CLI를 사용하는 방법, 그리고 Github 과 연동하거나, 컨테이너 레지스트리를 활용하는 방법 총 3가지가 있다. 여기서 필자는 Github을 활용해서 연동하는 방법을 소개해 보고자 한다.
heroku 에서 app을 생성하자." heroku 에서 app을 생성하자.  　﻿로컬에서 만든 애플리케이션을 Github에 push 하면 Github Repository 가 생기고 작업 파일들이 정상적으로 업로드된 것을 확인할 수 있다. 그다음 heroku에서 만들었던 App 페이지에서 Deploy 탭을 클릭하면 아래와 같이 3가지 방법으로 Deploy를 할 수 있다고 나오고, 이 중에 &ldquo;Connect to Github&quot;을 선택하면 Github 과 연동할 수 있는 버튼이 생기고 이를 누르면 자동으로 본인의 Github 내 Repository를 등록할 수 있도록 화면이 바뀐다.
heroku 와 github 연동" heroku 와 github 연동  　﻿그다음 위에서 Github에 push 했던 Repository 이름을 적고 검색하면 조회가 되고 &lsquo;Connect&rsquo;를 누르면 자동으로 연결이 된 것을 확인할 수 있다. 연동된 Repository 브랜치에 코드가 푸시 되면 자동으로 heroku에 배포가 되도록 자동화 설정도 가능하고 그 아래에 보면 브랜치를 선택해서 배포를 수동으로 할 수 있기도 하다. 수동으로 푸시를 눌러보면 이런저런 빌드 로그가 나오고 최종적으로 배포가 되어 {Appname}.herokuapp.com 을 접속해보면 서버에 배포가 되어있는 것을 확인할 수 있다.
 sample Github Code : taetaetae@heroku#HelloWorldController.java#L11 sample heroku app : https://taetaetae-test.herokuapp.com/  수동으로 배포를 해보자." 수동으로 배포를 해보자.  　﻿heroku에서는 이렇게 몇 번의 클릭만으로 간단하게 애플리케이션을 배포할 수 있는 기능을 제공하고 있고 서버 내 로그도 아래 화면처럼 보여주고 있기 때문에 쉽고 간단하게 서버를 구성하고 싶은 사용자들에게 매력적으로 보이는 것 같다. 단, 무료 플랜의 제한사항들을 자세히 살펴보고 사용할 것을 추천한다.
서버 로그도 볼 수 있다!" 서버 로그도 볼 수 있다!  호출 테스트, 문제의 시작 　﻿앞서 만들었던 텔레그램 채널에는 아무도 가입을 하지 않았기에 배포한 heroku web endpoint를 호출하면 텔레그램 봇을 통해 알림이 오는 걸 테스트하고 싶었다. 그런데 아무리 호출을 해도 서버는 타임아웃이라는 에러 응답을 뱉기 일쑤였고, 로직이 문제인지 한참을 리팩토링하며 원인을 파악하는데 꽤 오랜 시간을 삽질하였다. 왜 타임아웃이 발생할까? heroku는 web에서 바로 실행할 수 있는 console 페이지를 제공하고 있었다. 그래서 &lsquo;크롤링을 하기 위한 페이지&rsquo;와 &lsquo;구글&rsquo;을 비교하기 위해 단순하게 curl 해서 가져오는 테스트를 해보니 아래처럼 확연히 결과가 달랐다. 결국은 heroku 와 크롤링 하는 서버 간의 네트워크 타임아웃이 문제였던 것.
오류가 나고 원인을 찾는 과정이 가장 어려운 것 같다. 어플리케이션의 문제가 아닌 네트워크 자체의 문제" 오류가 나고 원인을 찾는 과정이 가장 어려운 것 같다. 어플리케이션의 문제가 아닌 네트워크 자체의 문제  그래서 어떻게 해결 했나? 　﻿heroku에서 타임아웃이 발생하는 문제를 해결하려 여러 구글링을 통해 찾아봤지만 방법을 찾을 수 없어서 결국 heroku를 사용하는 것을 포기하고 다른 방법을 찾아봐야만 했다.]]></description></item><item><title>공모주 알리미 개발 후기 - 2부</title><link>https://taetaetae.github.io/posts/public-offering-notice-2/</link><pubDate>Sun, 28 Mar 2021 11:41:33 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/public-offering-notice-2/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/public-offering-notice-2/logo.jpg" referrerpolicy="no-referrer">
            </div>﻿혹시 이 포스트를 처음 읽는 독자라면 지난 포스팅을 읽고 오는 것을 추천한다. 정리하자면, 지난 포스트에서는 토이 프로젝트를 시작하게 된 계기와, 어떤 식으로 만들지에 대한 설계. 그리고 데이터를 수집하는 과정에 대해 이야기했었다. 지난 포스팅에서 수집한 데이터를 이제 사용자들에게 알려주는 부분에 대해 정리하고자 한다.
 1부 : 프로젝트 설계, 데이터 수집 2부 : 수집한 데이터 알림 발송 3부 : 서버 선정 및 릴리즈  데이터 정의 　﻿java 라이브러리 중에 jsoup라는 것을 사용하여 웹사이트를 크롤링 하였고, 필요한 데이터를 파싱을 하였다. 아래는 &lsquo;공모주&rsquo;라는 자바 모델을 정의해 보았다. 이렇게 자바 &lsquo;모델&rsquo;로 정의를 하는 이유는 필요한 데이터가 무엇인지 다시 한번 정리를 하기 위함이기도 하고 map 같은 형태의 임시 변수(?)보다 더 직관적이기에 이후 코드를 작성하는데 가이드 역할의 효과도 얻을 수 있을 것 같았기 때문이다.
public class PublicOffering { private String name; // 종목명  private LocalDate startDate; // 일정 시작일  private LocalDate endDate; // 일정 마감일  private LocalDate listingDate; // 상장일  private String publicOfferingPrice; // 확정 공모가  private String expectedOfferingPrice; // 희망 공모가  private List&lt;String&gt; Underwriter; // 주간사  private String detailUrl; // 상세URL  private String competitionRate; // 청약경쟁률 } 　﻿초기에는 위에서 정의한 모델처럼 공모주의 기본 정보만을 서비스해야겠다 생각했고, 관련 뉴스라든지 기타 추가적인 정보나 다른 분들의 요구 사항(?)들이 추가될 경우 점진적으로 설계를 하고서 확장시켜 나가는 방향으로 계획했다. 우선은 기능들이 부족하더라고 돌아가는 서비스를 만들고 싶었기에.
애자일 방법론! 출처 : https://m.blog.naver.com/keycosmos3/221267522930" 애자일 방법론!
출처 : https://m.blog.naver.com/keycosmos3/221267522930  텔레그램 봇/채널 생성 　﻿텔레그램 봇을 만드는 과정은 가볍게 검색을 해보면 너무나 쉽게 찾을 수 있지만, 보다 하나의 글 안에 모든 내용을 담고 싶어 텔레그램 봇을 만들고 → 텔레그램 채널을 만든 다음 → 텔레그램 봇을 이용해서 텔레그램 채널에 메시지를 보내는 걸 이야기해 보고자 한다.
 ﻿아, 여기서 왜 꼭 &lsquo;텔레그램&rsquo;을 선택했는가에 대한 이유는 개인적으로 다른 메신저 (카카오톡, 라인 등)보다도 api를 활용하여 메시지를 보내는 과정이 단순하면서도 빠르고 쉽게 느껴졌기 때문이다. 혹시 텔레그램 이 아닌 다른 메신저로 보내달라는 요청이 있을 경우 그때 가서 고민해 보려 한다.
  텔레그램 봇 생성  　﻿먼저 텔레그램 메신저에서 &lsquo;BotFather&rsquo;라는 사용자를 찾고 &lsquo;/start&rsquo;를 누르면 아래와 같이 사용할 수 있는 명령어가 나온다.
　﻿그다음 우리는 봇을 만들 것이기 때문에 &lsquo;/newbot&rsquo;을 누르고 봇의 이름을 작성하고 그 봇의 사용자 이름을 지정한다. &lsquo;_bot&rsquo;으로 끝나야 한다고 하기에 이름 뒤에 붙여서 만들면 그걸로 끝. 다음으로 친절하게 HTTP API를 사용할 수 있는 토큰이 발급되는데 이 토큰으로 봇을 컨트롤 가능하기 때문에 잘 간직하고(?) 있어야 한다.
친절한 봇 아버지" 친절한 봇 아버지  　﻿이후 해당 토큰을 이용해서 봇의 상태를 확인해보자. 아래의 url에 토큰 경로만 변경하여 입력하면 json 응답을 받을 수 있다.
https://api.telegram.org/bot{token}/getUpdates e.g. https://api.telegram.org/bot17...42:AAH...cQU/getUpdates  텔레그램 채널 생성  　﻿1:N으로 채널에 가입한 사람들에게 메시지를 일방적으로 보내야 하기 때문에 사용할 채널을 만들어 보자. 텔레그램 UI만 봐도 간단하게 생성하기 쉽게 되어있다. 더불어 이 채널에 메시지를 보내야 하기 때문에 위에서 만들었던 봇을 추가하고 관리자로 승격 시키자.
채널 생성 &gt; 봇을 관리자로" 채널 생성 &gt; 봇을 관리자로   ﻿텔레그램 봇으로 텔레그램 채널에 메시지 보내기  　이 부분에서 약간 헤맸는데 결국 위에서 얻은 토큰과 채널의 특정 id를 알아야 메시지를 보낼 수 있다. 앞서 만들었던 채널에 아무 메시지나 작성을 하고 위에서 호출했던 &lsquo;getUpdates&rsquo; api를 다시 호출해보면 아래처럼 채널의 id를 구할 수 있게 된다.]]></description></item><item><title>공모주 알리미 개발 후기 - 1부</title><link>https://taetaetae.github.io/posts/public-offering-notice-1/</link><pubDate>Sun, 21 Mar 2021 19:13:49 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/public-offering-notice-1/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/public-offering-notice-1/logo.jpg" referrerpolicy="no-referrer">
            </div>﻿작년부터 시작된 &lsquo;동학 개미 운동&rsquo;에 언제부터인가 필자도 주린이로써 동참을 하게 되었다. 최근에는 &lsquo;공모주 청약&rsquo;이라는 걸 알게 되었는데 따라 해보고 정신 차려보니 치킨 한 마리 정도의 수익을 얻는 기적이 일어났다. 공모주란 정해진 일자에 청약을 하고 배정을 받으면 해당 주식이 상장을 하기 전에 미리 살 수 있다는 &lsquo;기회&rsquo;로 이해했다. (주린이라 이해의 범위가 여기까지다&hellip;) 공모주 배정이 로또처럼 엄청난 큰 수익률을 가져다주는 건 아니지만 앞서 이야기 한 것처럼 언제 있을지 모르는 공모주 청약을 꼬박꼬박 챙겨서 하게 된다면 맛있는 치킨을 먹을 수 있겠다는 기대감이 부풀었다. (치킨은 역시 교촌 허니콤보&hellip;)
주린이는 계속 자야 할까 싶다. 출처 : https://b-s-d.tistory.com/8" 주린이는 계속 자야 할까 싶다.
출처 : https://b-s-d.tistory.com/8  　﻿치킨이 머릿속에 맴도는 시간도 잠시. 필자의 머리를 스치는 하나의 생각. 그러면 공모주 청약은 언제 하는 거지? 청약하니까 준비하라고 누가 알려주면 좋을 텐데&hellip; 그러면서 이런저런 검색을 해보니 안드로이드 앱은 이미 있었고, IOS 앱은 없었다. 음? 그럼 이걸 내가 만들어보면 어떨까?
　﻿결론부터 말하자면, 텔레그램을 활용하여 자동화 공모주 알림봇을 만들게 되었다. 혹시 공모주에 관심이 있다면 필자가 만든 텔레그램 채널을 가입하는 것도 좋을 것 같다.﻿
　﻿이번 글에서는 필자의 새로운 토이 프로젝트인 &lsquo;공모주 알리미&rsquo;를 만들게 된 배경과 설계, 그리고 개발부터 릴리즈까지에 대해 이야기를 해보고자 한다. 크게 아래의 목차로 이야기하게 될 것 같다.
 1부 : 프로젝트 설계, 데이터 수집 2부 : 수집한 데이터 알림 발송 3부 : 서버 선정 및 릴리즈  　﻿자칫 너무 간단한데~, 이런 걸 굳이 왜 만들어?라는 시각이 있을 수 있겠지만 토이 프로젝트를 해야지 하고 마음을 먹었지만 막상 시작을 못하고 있는 어느 누군가에게는 도움이 될 내용인 것 같아서 꽤 자세히 정리를 하려 한다. 물론 이러한 정리는 필자 자신을 위해서가 더 크긴 하다.
프로젝트 설계 　﻿과거에 토이 프로젝트로 진행했던 기술블로그 구독 서비스의 경험을 되새기면서 처음부터 황소처럼 달려드는 것보단 충분에 충족을 더해 충만해질 때까지 고민을 오랫동안 해보기로 했다. (그래봤자 하루 정도&hellip;?^^) ﻿ 우선 데이터를 어딘가에서 가져오고 가져온 데이터를 DB에 저장할 것인지 아니면 저장하지 않고 휘발성으로 조회후 버리는(?) 형태로 할 것인지를 고민해야 했다. 공모주라는 데이터의 특성상 한번 정해진 메타 데이터가 상황에 따라 변경이 될 수도 있다고 했기에(일정이 변경되거나 공모가가 변경되거나 등) DB에 저장을 하게 되면 이를 동기화(Sync) 하는 비용이 추가로 생길 것 같아서 알림을 보내기 직전에만 조회하고 버리는 형태를 생각했다.
　그렇게 데이터를 조회했다면 이를 입맛에 맞게 가공하고서 사용자에게 알림을 줘야 한다. 알림을 발생시키는 방법은 매우 다양한데 뭔가 적은 비용으로 구성하고 싶었다. 즉, 알림을 받는 사용자가 10명, 100명, 1000명 이 되어도 (그렇게 될지는 모르겠지만;;) 내가 만든 서비스에서 알림 수신인이 늘어나는 경우를 고려하지 않아도 되었으면 했다. 그에 생각한 게 메신저 API. 그중에서도 텔레그램 API가 뭔가 이런 형식으로 딱일 것 같았기 때문이다. 결국 데이터를 메시지 형태에 맞춰 한 번만 발송하게 되면 1:N 형식(Broadcast)으로 텔레그램 채널을 구독하고 있는 사용자들에게 전송이 될 테니 안성맞춤이었다.
　그럼 언제 어떤 정보를 알려주는 게 좋을까? 청약이 보통 오전에 시작하기 때문에 대략 매일 오전 9시에 관련 정보들을 보내주면 될 것 같았다. 3일 전에 청약을 시작하게 되니 미리 준비하라는 알림. 그리고 청약 날짜가 도래해서 잊지 말고 청약을 신청하라는 알림. 마지막으로 공모주가 상장을 하게 되는 알림. 이 세 가지 알림만 잘 챙긴다면 필자 같은 주린이들도 충분히 공모주 청약으로 치킨을 먹을 수 있을 거라 생각했다.
　마지막으로 이 모든 내용을 개발한 어플리케이션을 어느 곳에 배포해야 하는지를 결정해야 했다. 항상 머릿속에는 있었지만 한 번도 안 해본 클라우드 Paas 인 heroku가 딱일 거라 생각했다.]]></description></item><item><title>Elastic Stack으로 코로나19 대시보드 만들기 - 2부 : 대시보드</title><link>https://taetaetae.github.io/posts/make-dashboards-from-elasticstack-2/</link><pubDate>Wed, 17 Feb 2021 16:53:49 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/make-dashboards-from-elasticstack-2/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/make-dashboards-from-elasticstack-2/logo.jpg" referrerpolicy="no-referrer">
            </div>지난 포스팅에서는 ﻿데이터를 수급하며 전처리 과정을 거쳤고, Filebeat와 Logstash를 거쳐 Elasticsearch에 인덱싱 하는 것까지 알아보았다. 앞선 포스팅에서 이야기했지만 단순하게 데이터를 시각화 도구를 이용해서 대시보드를 만드는 게 아니라 데이터가 추가되면 만들어둔 대시보드에 자동으로 반영되는 흐름을 만들고 싶었다. 마침 파이프라인을 이틀 전에 만들었기 때문에 그동안의 빠진 데이터를 추가해야 하는 상황이다. 이 경우 Filebeat-Logstash-Elasticsearch 가 실행 중이라면 앞서 작성했던 파이썬 스크립트만 한번 실행해 주면 이틀 치 데이터가 파이프라인을 거쳐 Elasticsearch로 인덱싱이 된다. 즉, 별도로 데이터를 가져와서 재 가공하고 추가하는 다소 까다로운 작업이 미리 만들어둔 파이프라인 덕분에 한 번의 스크립트 실행으로 손쉽게 처리가 됨을 알 수 있다.
　이제는 쌓여있는 데이터를 가지고 시각화를 해볼 차례이다. ElasticStack에서는 Kibana라는 강력한 시각화 도구를 제공하는데 이번 포스팅에서는 Kibana를 이용해서 대시보드를 만드는 방법에 대해 알아보려 한다.
Visualize 　﻿Elasticsearch에 인덱싱 되어있는 데이터들은 기본으로 제공되는 REST API를 통해서 조회할 수 있고 JSON 형태로 결과가 나오기 때문에 이를 가지고 다양하게 시각화를 할 수도 있다. 하지만 Kibana에서는 데이터를 조회하고 UI로 표현하는 일련의 모든 행위를 클릭 몇 번으로 할 수 있게 해주기 때문에 전문가가 아니더라도 조금만 만져보면 누구나 만들 수 있다.
New Visualizaion!!" New Visualizaion!!  　버전업이 되면서 비쥬얼라이즈를 만드는 첫 화면 또한 변화가 생겼다. 기존에는 어떤 유형의 비쥬얼라이즈를 선택할 것인지에 대해 선택하는 화면부터 나왔는데 만드는 걸 보다 편리하게 도와주는 Lens, TSVB 같은 기능들이 먼저 반겨준다. 이 기능을 통해서 만드는 방법도 괜찮지만 보다 명시적으로 만들고 싶으니 하단에 Aggregation based을 선택해서 원하는 비쥬얼라이즈의 타입을 선택해 보자. 이후 생성되어 있는 인덱스를 선택하면 본격적으로 비쥬얼라이즈를 그릴 수 있는 화면이 나오는데 대시보드 화면 기준으로 만들어야 할 항목별로 살펴보자.
전체 수 ." .  　﻿확진자, 사망자, 격리 해제의 총합을 표현하려 한다. 이렇게 &lsquo;숫자&rsquo;를 표현하려 하는 경우 Metric을 활용하곤 한다. 우측에서 Aggregation 방법을 &lsquo;sum&rsquo;으로 설정하고 필드는 유형별로 각각 선택해 주자. 아래 &lsquo;Add&rsquo;버튼을 눌러 확진, 사망, 격리 해제 수를 모두 표시한 다음 저장을 눌러준다. Label을 지정하지 않으면 어떤 형태로 Aggregation을 했는지를 Label 영역에 보여주는데 그게 보기 싫다면 원하는 텍스트로 지정해 주는 것도 방법이다.
최근 수 ." .  　﻿확진자, 사망자, 격리 해제의 &lsquo;최근 데이터&rsquo;를 보여주는 게 목적이다. 이 경우 Aggregation을 Top Hit으로 선택하면 필드를 선택할 수 있게 되는데 하루의 데이터가 총 18 row이기 때문에 (서울, 부산, &hellip;, 제주, 검역) 18 row 을 전부 더한 값이 하루 기준의 합계가 된다. 여기서 정렬을 날짜 기준 내림차순으로 해줘야 가장 최근 데이터의 합계가 되는 점도 신경 써야 한다.
각 타입별 합계 ." .  　﻿지역별로 타입별 수를 보기 위해 Pie 타입으로 선택하여 진행한다. 타입별(예로 들어 확진이면 confirmed)로 합계를 구하기 위해 Aggregation을 &lsquo;sum&rsquo;으로 설정하면 빈 원이 나오지만 각 지역별로 차트를 잘라서 봐야 하기에 하단의 Buckets의 Add를 누르고 regieon의 필드를 Terms Aggregation 한다. 18 row의 데이터가 전부 보여야 하기에 정렬 개수를 늘리고 option 탭에서 보는 취향에 알맞게 설정값들을 바꿔준다.
타입별 추이 ." .  　﻿확진, 사망, 격리 해제 중에 사망을 제외하고 나머지 둘은 데이터의 크기가 크고 변화량이 비슷하기 때문에 x축은 시간으로 설정해두고 사망은 막대로, 나머지 둘은 라인으로 한 화면에서 표현하면 이 3가지 데이터를 한눈에 보기 좋을 것 같았다. Vertical bar 을 선택하고 x축(Buckets &gt; X-axis)은 데이터 타입인 convert_date로 설정한다. 다음으로 사망은 매일 몇 명 사망했는지 뚜렷하게 보기 위해 그냥 sum으로, 나머지 둘은 누적 합계가 더 의미 있어 보일 것 같아 Cumulative Sum으로 Aggregation을 한다.]]></description></item><item><title>Elastic Stack으로 코로나19 대시보드 만들기 - 1부 : 파이프라인 구성</title><link>https://taetaetae.github.io/posts/make-dashboards-from-elasticstack-1/</link><pubDate>Mon, 15 Feb 2021 17:50:12 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/make-dashboards-from-elasticstack-1/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/make-dashboards-from-elasticstack-1/logo.jpg" referrerpolicy="no-referrer">
            </div>﻿　얼마 전에 필자의 블로그를 보고 어느 교육 기관에서 ElasticStack에 대한 강의 요청이 들어왔다. 사실 관련 기술에 대해 지식이 아주 깊고 해박한 게 아니라서 약간의 반감부터 들었지만 ElasticStack을 전혀 모르는 사람들 기준으로 어떻게 돌아가는지에 대해서만 간단하게 소개하는 정도로 하면 된다고 하여 조심스럽지만 떨리는 마음으로 열심히 준비를 하기 시작했다. 그런데, 이런저런 이유로 갑자기 강의를 할 수 없게 되었고 그간 준비했던 내용들이 너무 아쉽지만 아무 소용이 없게 되어버렸다. 그냥 중단하기엔 아쉬운 마음이 너무 커서 준비했던 내용 중에 &lsquo;데이터를 가지고 대시보드를 만드는 부분&rsquo;은 누군가에겐 도움이 될까 싶어 블로그에 정리를 해보려 한다.
 ﻿강의를 준비한 올해 1월 중순엔 Elasticsearch 버전이 7.10.2이었는데 블로그를 쓰고 있는 지금은 벌써 7.11으로 버전 업 되었다. 내가 아는 오픈소스 중에 버전업이 가장 빠른데 그렇다고 기능이 확 바뀌거나 습득하기 어렵게 바뀌진 않았다. 그만큼 사용자가 무엇을 원하는지 명확히 알고 작은 단위로 조금씩 바뀌어 가는 모습이 꽤 인상적이다.
 　﻿작년 초부터 코로나19 바이러스가 전 세계적으로 퍼지기 시작했고 아직까지도 진행 중이다. 나도 전염되는 건 아닐까 하는 두려움에 어디에서 얼마나 발생했는지를 확인하기 어렵던 시절 우리나라의 뛰어난 개발자들은 누가 시키지도 않았는데 정말 감사하게도 그 현황을 한눈에 볼 수 있도록 여러 유형으로 코로나19 바이러스 대시보드를 만들기 시작한다. 그 덕분에 좀 더 현황을 보기에 편해졌고 더욱 조심하게 되는 계기가 되었다고 생각한다. 이제는 포털사이트나 각종 매체를 통해 손쉽게 코로나19 바이러스의 현황을 볼 수 있지만 이러한 데이터를 가지고 검색엔진이지만 대시보드를 구축하는데 훌륭한 기능을 가지고 있는 ElasticStack을 활용해서 &lsquo;나만의 대시보드&rsquo;를 만드는 걸 정리해보고자 한다. 본 포스팅의 일회성으로 데이터를 가지고 대시보드를 만드는 것에서 끝나는 게 아니라 지속적으로 데이터가 업데이트된다는 가정하에 전반적인 &ldquo;파이프라인&quot;을 구축한 뒤 대시보드를 만들어 두고 데이터만 갱신하면 자동으로 대시보드 또한 업데이트되는 것을 목적으로 한다. 전체 흐름" 전체 흐름 
 ﻿글을 모두 작성하고 보니 양이 생각보다 길어져서 데이터를 조회하고 필터링하여 Elasticsearch에 인덱싱 하는 대시보드를 만들기 위한 일종의 &ldquo;데이터 파이프라인&quot;을 구성하는 부분과 만들어진 데이터 기반으로 Kibana의 다양한 기능을 활용하여 대시보드를 만드는 2개의 포스팅으로 나누어 정리해보겠다.
 ﻿　최종적으로 만들게 될 대시보드의 모습은 다음과 같다. 최종 목표!" 최종 목표! 
대시보드 구성 준비 　﻿예전에는 Elasticsearch, Logstash, Kibana 3가지를 가지고 ELK라 불리다 Beat라는 경량 수집기 제품이 등장하며 이 모든 걸 ElasticStack라 부르기 시작했다. (공식 홈페이지 참고) 먼저 어떤 목표와 어떤 순서로 대시보드를 구성할 것인지에 대해 정리해봐야겠다.﻿
데이터 　﻿데이터는 공공데이터 포털에서 가져오려다 조회를 해보니 누락되는 날짜도 있었고 원하는 데이터의 품질이 생각보다 좋지 않아서 다른 곳을 찾아봐야 했다. 그러다 간결하게 정리한 데이터가 깃헙에 공개가 되어 있어서 그것을 사용하려 한다. 해당 데이터는 https://coronaboard.kr/ 에서도 사용되는 데이터라고 한다. ﻿
데이터 전처리(preprocessing) 　﻿원하는 데이터는 위 깃헙에서 제공하는 데이터 중에 지역별 발생 현황. 해당 데이터를 살펴보면 요일별로 데이터가 &lsquo;누적&rsquo;되어 저장되어 있다. 즉, 서울지역 기준으로 2020년 2월 17일에 14명이 발생했고 2020년 2월 18일에 한 명도 발생하지 않았는데 14명으로 &lsquo;누적&rsquo;되어 저장되어 있다. 사실 이대로 해도 큰 문제는 없지만 어디까지나 별도의 가공 없이 최대한 원본 데이터(raw) 가 있어야 데이터 분석 시 다양하게 활용이 가능하기에 데이터를 분석하기 전에 전처리 과정이 필요했다. 정리하면, 집계 수가 누적되지 않고 날짜 기준으로 집계된 수만 있는 데이터를 원했다.﻿
　﻿필자는 주로 java를 가지고 개발을 하지만 가끔 간단한 스크립트성 개발은 python을 활용하는 편이기에 다소 이쁜 코드는 아니지만 데이터를 조작하려 아래와 같은 코드를 작성하였다.
import csv, requests import pandas as pd CSV_URL = &#39;https://raw.githubusercontent.com/jooeungen/coronaboard_kr/master/kr_regional_daily.csv&#39; # 확진, 사망, 격리해제 yesterday_data = {} yesterday_data[&#39;서울&#39;] = [0, 0, 0] yesterday_data[&#39;부산&#39;] = [0, 0, 0] yesterday_data[&#39;대구&#39;] = [0, 0, 0] yesterday_data[&#39;인천&#39;] = [0, 0, 0] yesterday_data[&#39;광주&#39;] = [0, 0, 0] yesterday_data[&#39;대전&#39;] = [0, 0, 0] yesterday_data[&#39;울산&#39;] = [0, 0, 0] yesterday_data[&#39;세종&#39;] = [0, 0, 0] yesterday_data[&#39;경기&#39;] = [0, 0, 0] yesterday_data[&#39;강원&#39;] = [0, 0, 0] yesterday_data[&#39;충북&#39;] = [0, 0, 0] yesterday_data[&#39;충남&#39;] = [0, 0, 0] yesterday_data[&#39;전북&#39;] = [0, 0, 0] yesterday_data[&#39;전남&#39;] = [0, 0, 0] yesterday_data[&#39;경북&#39;] = [0, 0, 0] yesterday_data[&#39;경남&#39;] = [0, 0, 0] yesterday_data[&#39;제주&#39;] = [0, 0, 0] yesterday_data[&#39;검역&#39;] = [0, 0, 0] flag = False csv_data = [] with requests.]]></description></item><item><title>Swagger와 Spring Restdocs의 우아한 조합 (by OpenAPI Spec)</title><link>https://taetaetae.github.io/posts/a-combination-of-swagger-and-spring-restdocs/</link><pubDate>Tue, 22 Dec 2020 10:41:40 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/a-combination-of-swagger-and-spring-restdocs/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/a-combination-of-swagger-and-spring-restdocs/main.jpg" referrerpolicy="no-referrer">
            </div>﻿MSA 환경에서의 API 문서화는 어떤 식으로 구성하는 걸까? 예컨대, 모듈이 10개 있다고 하면 각 모듈마다 API 문서가 만들어질 테고 API 문서를 클라이언트에 제공하기 위해서 각각의 (10개의) URL를 전달해야 할 텐데 이게 과연 효율적일까? 물론 기능별로 URL이 분리된다는 장점이 있고 굳이 모아보자면 각 API 문서를 다시 한번 크롤링 하여 검색할 수 있도록 제공하는 것도 하나의 방법이 될 수 있다. 하지만 이러한 방법들은 요구 사항을 위한 별도의 작업을 하게 되니 일을 위한 일이 되는 것 같아 뭔가 아쉬웠다. 좋은 방법이 없을까?
고민의 시작 　한창 궁금증이 머릿속에서 지워지지 않았을때 Spring 한국 스프링 사용자 모임 페이스북 그룹에 문의도 해가며 방법을 찾아가고 있었다.
﻿닉네임이나 프로필 사진은 그들의 개인 정보를 위해 임의로 지정하였다." ﻿닉네임이나 프로필 사진은 그들의 개인 정보를 위해 임의로 지정하였다.  　﻿필자와 함께 개발자의 인생을 시작한 멋진 친구들에게 정확히 올해 6월 초에 고민을 털어놓으며 좋은 방법이 없을지에 대한 논의를 했던 적이 있다. 그런데 친구 중 한 명이 잊고 있었던 그 이슈에 대해서 다시 꺼내며 URL 하나를 던져준다. 참 고마운 친구들.
 Shout out 34. asuraiv, black9p
 언 반년이 지났으나 필자도 잊고 있었던 이슈를 그는 기억하고 있었다." 언 반년이 지났으나 필자도 잊고 있었던 이슈를 그는 기억하고 있었다.  　﻿올해 NHN FORWARD에서 진행했던 세션 중에서 MSA 환경에서 API 문서 관리하기: 생성부터 배포까지라는 제목의 내용이었고, 정확하게 필자가 고민했던 부분을 콕! 집어서 해결해 준 사례였다. 역시 세상엔 엄청난 고수들이 내가 고민했던 부분들을 이미(혹은 이후에라도) 고민하고 해결한 경우가 많다는 것을 느끼고 공유의 힘이 이렇게도 대단하구나 하며 놀라움을 금치 못하였다.﻿
  　﻿이번 포스팅에서는 OpenAPI Spec 을 활용하여 Spring Restdocs로 만들어지는 문서를 Swagger UI에서 보는 흐름을 실제로 구현해 보고자 한다. 즉, Swagger 나 Spring Restdocs 뭐로 만들든 간에 OpenAPI Spec에 맞춰서만 만든다면 한곳에서 볼 수 있겠다는 희망이 보였다. 며칠 전 작성한 OpenAPI 와 Swagger-ui 포스팅을 본 독자들은 지금의 포스팅을 작성하기 위한 밑거름이었다는 사실을 눈치챘을 수도 있을 것 같다.
 ﻿좋은 내용을 공유해 주신 (저의 고민을 완벽하게 해결해 주신) NHN FORWARD 발표자분께 이 포스팅을 빌어 감사의 인사를 보냅니다. :) 당장 팀 내에도 적용해봐야겠어요!!
 ﻿Spring Restdocs에서 OpenAPI Spec 추출 　﻿누가 또 친절하게 오픈소스로 만들어놨다. https://github.com/ePages-de/restdocs-api-spec 에서 관련 내용을 확인할 수가 있는데 해당 링크에서는 gradle 버전이고 https://github.com/BerkleyTechnologyServices/restdocs-spec 는 maven 버전이라고 한다. 마침 필자의 Github에 Maven 버전으로 SpringRestdocs를 세팅해둔 Repository 가 있어서 이를 활용해보고자 한다.﻿
pom.xml 추가 　﻿관련 dependency를 추가하자. jcenter라고 bintray.com 에서 운영되는 Maven Repository에 올려진 오픈소스이니 repository 도 추가해 주자.
&lt;properties&gt; &lt;restdocs-api-spec.version&gt;0.10.0&lt;/restdocs-api-spec.version&gt; &lt;restdocs-spec.version&gt;0.19&lt;/restdocs-spec.version&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jcenter&lt;/id&gt; &lt;url&gt;https://jcenter.bintray.com&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependency&gt; &lt;groupId&gt;com.epages&lt;/groupId&gt; &lt;artifactId&gt;restdocs-api-spec&lt;/artifactId&gt; &lt;version&gt;${restdocs-api-spec.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.epages&lt;/groupId&gt; &lt;artifactId&gt;restdocs-api-spec-mockmvc&lt;/artifactId&gt; &lt;version&gt;${restdocs-api-spec.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; ﻿위의 dependency에서 제공해 주는 모듈로 테스트의 SpringRestdocs를 만들었다면 OpenAPI Spec 을 만들어 주는 plugin 또한 추가해 주자
&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;jcenter&lt;/id&gt; &lt;url&gt;https://jcenter.bintray.com&lt;/url&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;plugin&gt; &lt;groupId&gt;com.github.berkleytechnologyservices.restdocs-spec&lt;/groupId&gt; &lt;artifactId&gt;restdocs-spec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${restdocs-spec.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;generate&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;specification&gt;OPENAPI_V3&lt;/specification&gt; &lt;format&gt;JSON&lt;/format&gt; &lt;outputDirectory&gt;${project.build.directory}/classes/static/docs&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; ﻿위 plugin 설정을 보면 format 을 JSON으로 한 것을 볼 수 있는데 YAML로도 만들 수 있다. 자세한 사용방법은 위에서 명시한 링크를 참고해보는 게 좋을 것 같다.
문서화 로직 추가 　﻿기존에 SpringRestdocs를 작성하는 로직은 org.springframework.restdocs.mockmvc.MockMvcRestDocumentation에서 제공해 주는 메서드를 사용했지만 위에서 이야기 한 오픈소스를 사용하기 위해 com.epages.restdocs.apispec.MockMvcRestDocumentationWrapper를 사용하도록 하자. 변경을 최소화하기 위해 import만 변경하도록 한다.
//import static org.springframework.restdocs.mockmvc.MockMvcRestDocumentation.*; import static com.epages.restdocs.apispec.MockMvcRestDocumentationWrapper.*; ﻿위와 같이 설정하고 Maven 빌드를 해보면 plugin에서 지정한 경로에 JSON 파일이 생성된 것을 확인할 수 있다.]]></description></item><item><title>OpenAPI 와 Swagger-ui 적용하기</title><link>https://taetaetae.github.io/posts/openapi-and-swagger-ui-in-spring-boot/</link><pubDate>Sun, 20 Dec 2020 11:42:40 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/openapi-and-swagger-ui-in-spring-boot/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/openapi-and-swagger-ui-in-spring-boot/main.png" referrerpolicy="no-referrer">
            </div>﻿API를 개발하고 사용방법에 대한 명세를 작성하는 방법은 여러 가지가 있다. 가장 심플하게 개발 코드와는 별도로 직접 수기로 작성하여 파일 혹은 문서 링크를 전달하는 방법이 있다. 하지만 개발 코드와 별도로 직접 작성을 한다는 점에서 오타/실수가 발생할 수 있고 최신화가 안되는 여러 가지 문제가 발생한다. 그에 등장한 API 문서화 자동화 툴의 양대 산맥인 SpringRestDocs 와 Swagger.
　﻿과거 SpringRestDocs 에 대한 포스팅을 했기에 이번엔 Swagger에 대한 사용방법에 대해 정리해보고자 한다. 이 둘의 장단점은 너무 뚜렷하기에 API문서를 제공하는 상황에 따라 적절하게 선택하여 사용할 수 있었으면 좋겠다.
﻿SpringBoot에 Swagger 적용 　기본 SpringBoot 가 셋팅되어 있다는 가정하에 Swagger 관련 dependency를 추가해주자. 아참, 이제부터의 프로젝트 셋팅은 Gradle로 하려한다. (물론 Maven으로 해도 무방하지만&hellip;)
dependencies { implementation &#34;io.springfox:springfox-boot-starter:3.0.0&#34; } 　﻿이후 JavaConfig 을 아래와 같이 설정하는데 아래 내용은 아주 기본 세팅이니 자세한 내용은 공식 도큐문서를 참고해 보면 좋을 것 같다. (물론 샘플 프로젝트를 만들며 필요할 것 같은 내용은 아래에서 설명하겠다.)﻿
@EnableSwagger2 @Configuration public class SwaggerConfig { @Bean public Docket api() { return new Docket(DocumentationType.SWAGGER_2) .select() .apis(RequestHandlerSelectors.any()) .paths(PathSelectors.any()) .build(); } } 　﻿테스트할 컨트롤러를 아래처럼 심플하게 작성하고(사칙연산&hellip;) 실행을 시킨 후 /swagger-ui/에 접속을 해보면 swagger 관련 javaConfig 하나만 추가했는데 문서가 만들어진 것을 확인할 수 있다. (http method는 편의상 다양하게 작성했으니 왜 DELETE 인가라는 의문은 접어두자.)
@RestController public class SampleController { @GetMapping(value = &#34;/addition&#34;) public Integer addition(Integer num1, Integer num2) { return num1 + num2; } @PostMapping(value = &#34;/subtraction&#34;) public Integer subtraction(Integer num1, Integer num2) { return num1 - num2; } @PutMapping(value = &#34;/multiplication&#34;) public Integer multiplication(Integer num1, Integer num2) { return num1 * num2; } @DeleteMapping(value = &#34;/division&#34;) public Integer division(Integer num1, Integer num2) { return num1 / num2; } } 기본 셋팅만 했는데 이런 화면이 나타났다." 기본 셋팅만 했는데 이런 화면이 나타났다.  　﻿위에서 했던 설정들 중 몇 가지만 좀 더 자세히 살펴보자.
   설정 설명     Docket ﻿Springfox 프레임 워크의 기본 인터페이스가 될 빌더로 구성을 위한 여러 가지 기본값과 편리한 방법을 제공하고 있다. 이후 select()로 ApiSelectorBuilder를 반환받아 사용할 수 있도록 해준다.   apis ﻿어떤 위치에 있는 API들을 가져올 것인가에 대한 정의. RequestHandlerSelectors.any()이라고 했으니 SpringBoot에서 기본으로 제공하는 basic-error-controller 도 API 문서로 만들어진 것을 확인할 수 있다. 특정 패키지만 적용하기 위해서는 RequestHandlerSelectors.basePackage(&quot;com.taetaetae.swagger.api&quot;) 와 같은 형식으로 지정하면 해당 패키지 하위에 있는 Controller를 기준으로 문서를 만들어 준다﻿.   paths ﻿이름에서도 눈치를 챌 수 있듯이 특정 path만 필터링해서 문서를 만들어 준다.   useDefaultResponseMessages ﻿기본 http 응답 코드를 사용해야 하는지를 나타내는 플래그    ﻿이외에도 security 나 공통으로 사용되는 파라미터 등 다양한 옵션을 설정할 수 있으니 가능하면 상황에 맞게 설정을 변경해 보는 것도 좋을 것 같다. 다른 설정들을 추가시켜서 좀 더 친절하게 만들어 보면 아래처럼 만들 수 있고 해당 코드는 Github에서 확인 가능하다.
API 문서화는 최대한 친절하게!!" API 문서화는 최대한 친절하게!!  OpenAPI 　﻿Swagger 공식 홈페이지를 이리저리 둘러보면 OpenAPI라는 내용이 많이 나온다. 그렇다면 OpenAPI는 무엇일까? 문서에 나와있는 내용을 직역해보면 Swagger 사양으로 알려져 있으며 RESTful 웹 서비스를 설명, 생성, 소비 및 시각화하기 위한 기계 판독 가능 인터페이스 파일에 대한 사양이라고 한다. 즉, API 자체를 설명하는 인터페이스 스펙이라고 이해를 해볼 수 있다. 위에서 만들어졌던 Swagger를 보면 http://localhost:8080/v2/api-docs?group=Test API 라고 나와있는데 이를 클릭해보면 아래와 같이 json 형태로 보인다.]]></description></item><item><title>Jenkins Job을 병렬로 실행해서 속도를 개선해보자. (by. Pipeline)</title><link>https://taetaetae.github.io/posts/jenkins-job-parallel-processing-by-pipeline/</link><pubDate>Sun, 06 Dec 2020 20:19:47 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/jenkins-job-parallel-processing-by-pipeline/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/jenkins-job-parallel-processing-by-pipeline/pipeline.jpg" referrerpolicy="no-referrer">
            </div>﻿　관리하는 URL이 200응답을 주고 있는지 모니터링을 한다고 가정해보자. 다양한 방법이 생각나겠지만 가장 처음으로 떠오른 건 단연 Jenkins. 간단하게 사용할 언어에 맞춰 Execute Script를 작성하고 스케줄링을 걸어 놓으면 큰 수고 없이 모니터링을 구성할 수 있게 된다. 아래는 python script로 작성해 보았다.
import requests url=&#34;http://모니터링url&#34; status_code = requests.get(url).status_code if status_code != 200: print(f&#39;응답 실패 :{url}, status : {status_code}&#39;) exit(1) ﻿　하지만 모니터링을 해야 하는 URL이 1개에서 여러 개로 늘어난다면 어떻게 될까? 단순하게 작성한 Script를 아래처럼 약간 수정하면 되긴 하지만 URL마다 응답속도가 다를 경우 순차적으로 실행하다 보니 실행 속도는 느릴 수밖에 없다.
import requests urls = [ &#34;http://모니터링url-1&#34;, &#34;http://모니터링url-2&#34;, &#34;http://모니터링url-3&#34; ] for url in urls: status_code = requests.get(url).status_code if status_code != 200: print(f&#39;응답 실패 :{url}, status : {status_code}&#39;) exit(1) ﻿ 이러한 경우, 빠른 속도를 보장하기 위해서는 병렬로 실행을 해야 한다는 건 누구나 다 알지만 그렇다고 Thread를 사용하기엔 벌써부터 덜컥 부담이 된다. 그렇다고 Job을 URL 개수만큼 늘리기에는 배보다 배꼽이 더 커버리고&hellip; 그러다 발견한 기능이 바로 Jenkins Pipeline!
　이번 포스팅에서는 Jenkins Job을 동시에 여러 번 사용해야 하는 경우를 Pipeline을 통해서 개선한 내용에 대하여 공유해보려 한다. Jenkins Pipeline에 대해 들어만 봤는데 이번에 실제로 사용해보니 생각보다 쉽게 개선할 수 있었고 옵션들을 상황에 맞게 조합을 잘 한다면 상당히 활용성이 높아 보이는 기능인 것 같다.
기존상황 　테﻿스트를 위해 임의로 느린 응답을 생성하도록 URL을 구성하고 위에서 이야기했던 것처럼 Job 하나에 아주 심플하게 Python script를 작성하고 실행해보도록 하자. 임의로 느린 응답은 http://slowwly.robertomurray.co.uk/ 에서 제공하는 기능을 활용하였다.﻿
import requests urls = [ &#34;http://slowwly.robertomurray.co.uk/delay/0/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/100/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/200/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/500/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/1000/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/2000/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/5000/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/10000/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/20000/url/https://www.naver.com/&#34; ] for url in urls: status_code = requests.get(url).status_code if status_code != 200: print(f&#39;응답 실패 :{url}, status : {status_code}&#39;) exit(1) print(f&#39;응답성공 : {url}&#39;) 그래서 실행해보면 50초가 소요되었다. 자, 이제 개선을 해보자!
개선을 해보자 　﻿전체적인 개선의 흐름은 하나의 Job에 모니터링하고자 하는 url을 파라미터로 받아서 처리할 수 있도록 설정하고, 이를 Jenkins Pipeline 을 통해 여러 URL을 동시에 모니터링하게 구성하는 것이다. 그러면 두 개의 Job(파라미터로 받아 모니터링하는 Job, Jenkins Pipeline Job) 만으로 보다 빠르고 효율적인 구성을 할 수 있을 것으로 상상을 하고.
Job을 범용적으로 (Jenkins paramters 활용) 　﻿위에서 샘플로 작성하였던 Python script는 url 이 늘어날수록 Job 안에 script를 수정해야 한다. 그렇게 해도 무방하지만 이번 개선의 목표는 하나의 Job을 Pipeline 이 병렬로 컨트롤하도록 설정해야 했기 때문에 Jenkins Job에 파라미터를 받을 수 있도록 아래처럼 Jenkins Job 설정에 파라미터를 설정하고 Python script 또한 수정해 주자.
﻿Job &gt; 구성 &gt; 이 빌드는 매개변수가 있습니다" ﻿Job &gt; 구성 &gt; 이 빌드는 매개변수가 있습니다  import requests, os url = os.environ[&#39;url&#39;] status_code = requests.get(url).status_code if status_code != 200: print(f&#39;응답 실패 :{url}, status : {status_code}&#39;) exit(1) print(f&#39;응답성공 : {url}&#39;) 병렬 실행을 위한 Jenkins 설정 　﻿Jenkins Job 을 생성하면 기본적으로 Job마다의 대기열(Queue)이 있어 Job이 실행 중이라면 시작된 시간 순서대로 기다렸다가 앞선 Job이 종료가 되면 이어서 실행되는 구조이다. 하지만 우리는 Job을 병렬로 실행해야 했기에 Job 설정 중 필요한 경우 concurrent 빌드 실행 옵션을 켜줘서 기다리지 않고 병렬로 실행될 수 있도록 해준다.
﻿Job &gt; 구성 &gt; 필요한 경우 concurrent 빌드 실행" ﻿Job &gt; 구성 &gt; 필요한 경우 concurrent 빌드 실행  　﻿또한 Jenkins Job 자체는 병렬로 실행되도록 설정되었다 해도 기본적으로 Jenkins 자체의 대기열은 한정되어 있기 때문에 적당히 늘려줘서 여러 개의 Job이 대기 열 없이 동시에 실행될 수 있도록 해준다.]]></description></item><item><title>기술블로그 개편기 (by HUGO)</title><link>https://taetaetae.github.io/posts/blog-reorganization-by-hugo/</link><pubDate>Sun, 29 Nov 2020 18:12:15 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/blog-reorganization-by-hugo/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/blog-reorganization-by-hugo/hexo_to_hugo.png" referrerpolicy="no-referrer">
            </div>웹서비스 개발자라면 나만의 블로그쯤은 있어야지 하며 기술 블로그를 시작한 지도 어느덧 4년이 되었다. 처음엔 그저 새로 알게 된 기술이나 삽질하며 경험한 것들 중에 핵심만을 적어놓는 수준이었다. (지금 다시 보면 뭔가 오글거리는 건 기분 탓이겠지&hellip;) 그렇게 계속 글을 써오면서 글쓰기라는 것에 관심을 갖게 되고 내 글이 누군가에게 도움이 될 거라는 기대에 조금이라도 글을 잘 써보고자 단순 기록 용이 아닌 하나의 &lsquo;글&rsquo;을 쓰려고 노력해 온 것 같다.
　일주일에 한 개는 써야지. 한 달에 한 개는 써야지. 하며 자꾸 나 자신과의 타협을 하다가 최근에는 회사에서 운영하는 서비스 개편 때문에 정신없이 바쁘다는 핑계로 &lsquo;블로그&rsquo;에 &lsquo;ㅂ&rsquo;자도 생각하지 못하게 된다. 무엇이 문제일까?라는 생각은 결국 내 기술 블로그도 회사 서비스처럼 &lsquo;개편&rsquo;을 해보자는 생각으로 도달하게 되었고 간단할 것만 같았던 기술 블로그 개편 작업은 꽤 오랫동안 + 다양한 삽질들로 작업을 하게 된다.
　이번 포스팅에서는 기술 블로그를 개편하며 겪었던 내용들에 대해 정리해보고자 한다. 기존에 기술 블로그를 운영하시는 분들이나 이번에 새롭게 시작하시는 분들께 도움이 될 거라 기대한다. 더불어 서비스 &lsquo;출시&rsquo; 가 아닌 개편&rsquo;이라는 과정 속에서 느끼게 되었던 인사이트도 간략하게 작성해볼까 한다.
기술블로그 플랫폼 선택 　처음 블로그를 쓰기 시작했을 때 포털서비스의 글쓰기 플랫폼을 사용하지 않은 이유는 단 하나다. &lsquo;글쓰기&rsquo; 뿐만 아니라 개발자이기에 웹사이트(블로그)를 내 입맛에 맞게 커스터마이징 하기 위해서. 그 이유로 hexo 라는 프레임워크에 github의 호스팅을 사용하여 운영을 해왔다. 그렇게 블로그를 운영해오면서 느꼈던 불편했던 부분들과 개편을 하며 기대하는 부분들을 정리하면 아래와 같다.
 테마(UI)가 이뻐야 하고 기능들이 많으면 좋겠다. 기술 블로그인 만큼 코드가 많이 삽입되니 코드 표현 또한 이뻐야 한다. 테마 또는 프레임워크의 커뮤니티가 활발해야 한다. 페이지 생성 또는 만들어진 웹페이지의 성능이 좋아야 한다. 글을 작성하고 배포하는 과정이 심플하고 깔끔해야 한다. ﻿  　위와 같은 이유를 기반으로 검색을 해보다 SSG(쓱 쇼핑몰 아님, Static site generators)를 깔끔하게 정리해 놓은 사이트를 발견한다. 정말 다양한 플랫폼들을 살펴보며 필자에게 맞는 게 어떤 건지 고민하다 결국 hugo 를 선택하게 된다. hugo를 선택한 이유는 go라는 언어를 사용한다는 것과 (간접적으로라도 다른 언어를 경험해보고 싶어서 + go 언어가 빠르다는 소리를 어디선가 들어서) 테마들이 너무 다양했기 때문이다.
﻿아주 대놓고 빠르다고 하니&hellip; 쓰고 싶어진다." ﻿아주 대놓고 빠르다고 하니&hellip; 쓰고 싶어진다.  　결국 hugo에 hugo-ranking-trend라는 사이트에서 상위에 랭크가 되어있고 기술 블로그 성격에 적합할 것 같은 LoveIt이라는 테마를 사용하기로 결정하였다. 자 그럼 시작해볼까?!
hugo 는 어떻게 쓰는거야? ﻿　대부분의 오픈소스는 hello world 혹은 quick start 같이 처음 접하는 사람들을 위한 도큐먼트가 있기 마련. hugo도 마찬가지로 quick-start가 있었고 이를 천천히 따라 하면 생각보다 쉽게 초기 세팅을 할 수 있었&hellip; 을꺼라 기대했지만 약간 초기 설정 과정이 어려워서 남겨 두고자 한다.
 참고로 필자는 윈도 10 환경에서 구성하였다. mac이라면 더 쉽게 설정할 수 있는 것 같은데 이 부분은 OS의 차이에서 생겨나는 어쩔 수 없는 약간의 장벽이라 생각한다. 이쁜 테마와 새로운 환경을 사용할 수 있다는 기대감으로 꾹 참아본다.
 기본설정 　﻿git이 설치되어 있다는 가정하에 우선 hugo는 go 언어기반으로 돌아가기에 우선 go를 설치해야 한다. 다운로드페이지에서 환경에 맞는 설치 파일을 다운로드하고 설치를 해준다. 다음으로 패키지 관리자인 chocolatey 또한 설치가 필요하다. 공식 홈페이지페이지에서 나와있는 순서대로 진행하면 설치 완료. 필자는 여기서 진행이 잘 안됐었는데, &lsquo;관리자 권한&rsquo;으로 PowerShell 을 실행시켜야지만 성공을 할 수 있었다.﻿
﻿　위 설정이 완료되었으면 드디어 hugo를 설치해 주고 초기화를 해준 뒤 샘플로 글 하나를 만들고 서버를 띄우면 끝.
# chocolatey 에 의해 hugo 설치 choco install hugo -confirm # hugo 초기화 hugo new site quickstart # post 생성 hugo new posts/post-name.]]></description></item><item><title>빌드/테스트는 내가 해줄게. 너는 코딩에 집중해 (by GitHub Pull Request Builder)</title><link>https://taetaetae.github.io/2020/09/07/github-pullrequest-build/</link><pubDate>Mon, 07 Sep 2020 10:09:56 +0000</pubDate><author>Author</author><guid>https://taetaetae.github.io/2020/09/07/github-pullrequest-build/</guid><description><![CDATA[git 은 분산 버전 관리 시스템 중 가장 잘 알려져 있다고 해도 과언이 아닐 정도로 대부분의 시스템에서 사용되고 있는 것 같다. 이를 웹서비스에서 보다 편하게 사용할 수 있도록 한 시스템이 Github. Github 을 사용하는 이유 중에 가장 큰 이유를 하나만 이야기해보자면 바로 온라인상에서 코드 리뷰를 할 수 있는 pullRequest라는 기능 때문이 아닐까 조심스럽게 생각을 해본다.
　pullRequest는 work branch에서 작업한 내용을 base branch로 merge 전 꼭 코드 리뷰가 아니더라도 작업한 내용에 대해서 다양한 검사를 자동화할 수 있는 강력한 기능들이 많다. 이러한 자동화는 CI(지속적 통합) 관점에서 매우 중요한데 코드에 대해 체크해야 할 부분들(빌드, 테스트, 정적 분석 등)을 &ldquo;알아서&rdquo; 해준다면 작업자는 오롯이 비즈니스 로직 개발에 대해서만 신경 쓸 수 있으니 생산성 절약 측면에서 엄청난 효과를 볼 수 있다.
내가 하는일에만 집중할 수 있게! 출처 : https://www.clien.net/service/board/park/10453442" 내가 하는일에만 집중할 수 있게! 출처 : https://www.clien.net/service/board/park/10453442  이번 포스팅에서는 그중에서도 아주 간단한 설정만으로 work branch의 빌드 상태를 검사해 볼 수 있는 Jenkins의 Github Pull Request Builder를 설치 및 활용해 보고자 한다.
 사실 최근 팀에서 CI 서버를 이전해야 했었다. 머릿속에서는 어떻게 하면 되겠지 싶었지만 막상 해보려니 Jenkins 버전업도 되었고 뭐부터 해야 할지 허둥대는 필자가 부끄러웠다. 이참에 정리를 해보며 다시 한번 리마인드 하는 시간을 가져보고자 한다. (이래서 기억보다 기록이 중요하다.)
 준비물 　전체적인 흐름은 아래 그림처럼 흘러가기 때문에 당연히 서버에 Jenkins 가 설치되어 있어야 한다. Jenkins 설치는 필자의 포스팅(Jenkins 설치 치트키)를 참고해 보는 것도 좋을 것 같다.
전체적인 흐름" 전체적인 흐름  　참고로 필자는 GitHub Enterprise 버전에서 사용했는데 일반 Github에서도 동일한 방법으로 사용 가능하다.
Github과 Jenkins의 연동을 위한 2가지 설정 　Github 과 Jenkins 가 통신이 되도록 설정해 줘야 한다. 그래야 Github의 코드를 받아서 Jenkins 가 빌드를 하고 그 빌드 결과를 다시 Github에 리포트가 가능해지기 때문이다. 먼저 첫 번째로 ssh 설정으로 Github의 코드를 가져오도록 ssh 설정을 해두자. ssh 설정하는 방법은 필자의 포스팅(Github과 Jenkins 연동하기)편을 확인해보면 될 것 같다.
　그다음으로 아래에서 이야기할 GitHub Pull Request Builder라는 Jenkins plugin 이 빌드가 끝난 뒤에 결과를 리포팅 해줄 수 있는 인증 토큰을 발급받아두자. Github &gt; Settings &gt; Developer settings &gt; Personal access tokens 화면에서 키를 생성하고 만들어진 키를 저장해 둔다. (이 키는 보안에 유의해야 하고, 화면 경고(?)에서도 볼 수 있듯이 키는 생성 시 한 번밖에 볼 수 없기 때문에 미리 저장해 둬야 한다.)
인증토큰을 미리 받아두자." 인증토큰을 미리 받아두자.  Jenkins 설정 　Jenkins &gt; 관리 &gt; pluginManager에 들어가 GitHub Pull Request Builder를 검색 후 설치해 준다. 그러고 나서 Jenkins &gt; 관리 &gt; 환경설정에 들어가 보면 아래와 같이 GitHub Pull Request Builder 항목이 생긴 것을 확인할 수 있고 위에서 설정한 인증토큰을 아래처럼 등록 후 저장을 한다.
credentials 을 위에서 발급받은 인증토큰으로 등록해준다." credentials 을 위에서 발급받은 인증토큰으로 등록해준다.  　Jenkins job을 하나 만들고 pullRequest 가 발생했을 때 자동으로 실행될 수 있도록 설정을 해준다. 먼저 General 탭에 Github project에 Github url 을 적어주고
 　소스 코드 관리 탭에서 ssh 주소를 적고 위에서 미리 설정한 ssh 키로 credentials 값을 넣어준다. 전에도 이야기했지만 이 부분에서 오류가 발생하면 빨간색 글씨로 오류 내용이 나오고 아래 화면처럼 오류가 없다면 아무것도 안 나온다. Refspec 에 +refs/pull/*:refs/remotes/origin/pr/* 라고 적어주고 브랜치 설정은 파라미터로 받아와서 pullRequest를 발생시킨 브랜치를 빌드 할 수 있도록 ${sha1} 라고 적어주자.]]></description></item></channel></rss>