<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Tech - Category - 👨‍💻꿈꾸는 태태태의 공간</title><link>https://taetaetae.github.io/categories/tech/</link><description>Tech - Category - 👨‍💻꿈꾸는 태태태의 공간</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 21 Mar 2021 19:13:49 +0900</lastBuildDate><atom:link href="https://taetaetae.github.io/categories/tech/" rel="self" type="application/rss+xml"/><item><title>공모주 알리미 개발 후기 - 1부</title><link>https://taetaetae.github.io/posts/public-offering-notice-1/</link><pubDate>Sun, 21 Mar 2021 19:13:49 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/public-offering-notice-1/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/public-offering-notice-1/logo.jpg" referrerpolicy="no-referrer">
            </div>작년부터 시작된 &lsquo;동학개미운동&rsquo;에 언제부터인가 필자도 주린이로써 동참을 하게 되었다. 최근에는 &lsquo;공모주 청약&rsquo; 이라는걸 알게 되었는데 따라해보고 정신차려보니 치킨 한마리정도의 수익을 얻는 기적이 일어났다. 공모주란 정해진 일자에 청약을 하고 배정을 받으면 해당 주식이 상장을 하기 전에 미리 살 수 있다는 &lsquo;기회&rsquo;로 이해했다. (주린이라 이해의 범위가 여기까지다&hellip;) 공모주 배정이 로또처럼 엄청난 큰 수익률을 가져다 주는건 아니지만 앞서 이야기 한것처럼 언제 있을지 모르는 공모주 청약을 꼬박꼬박 챙겨서 하게 된다면 맛있는 치킨을 먹을 수 있겠다는 기대감이 부풀었다. (치킨은 역시 교촌 허니콤보&hellip;)
주린이는 계속 자야 할까 싶다. 출처 : https://b-s-d.tistory.com/8" 주린이는 계속 자야 할까 싶다.
출처 : https://b-s-d.tistory.com/8  　치킨이 머릿속에 맴도는 시간도 잠시. 필자의 머리를 스치는 하나의 생각. 그러면 공모주 청약은 언제하는거지? 청약 하니까 준비하라고 누가 알려주면 좋을텐데&hellip; 그러면서 이런저런 검색을 해보니 안드로이드 앱은 이미 있었고, IOS 앱은 없었다. 음? 그럼 이걸 내가 만들어보면 어떨까?
　결론부터 말하자면, 텔레그램을 활용하여 자동화 공모주 알림봇을 만들게 되었다. 혹시 공모주에 관심이 있다면 필자가 만든 텔레그램 채널을 가입하는것도 좋을것 같다.
　이번 글에서는 필자의 새로운 토이 프로젝트인 &lsquo;공모주 알리미&rsquo;를 만들게 된 배경과 설계, 그리고 개발부터 릴리즈까지에 대해 이야기를 해보고자 한다. 크게 아래의 목차로 이야기 하게 될것 같다.
 1부 : 프로젝트 설계, 데이터 수집 2부 : 수집한 데이터 알림 3부 : 서버 선정 및 릴리즈  　자칫 너무 간단한데~, 이런걸 구지 왜 만들어? 라는 시각이 있을수 있겠지만 토이프로젝트를 해야지 하고 마음을 먹었지만 막상 시작을 못하고 있는 어느 누군가에게는 도움이 될 내용인것 같아서 꽤 자세히 정리를 하려 한다. 물론 이러한 정리는 필자 자신을 위해서가 더 크긴 하다.
프로젝트 설계 　과거에 토이프로젝트로 진행했던 기술블로그 구독서비스 의 경험을 되새기면서 처음부터 황소처럼 달려드는것보단 충분에 충족을 더해 충만해 질때까지 고민을 오랬동안 해보기로 했다. (그래봤자 하루정도&hellip;?^^)
　우선 데이터를 어딘가에서 가져오고 가져온 데이터를 DB에 저장할 것인지 아니면 저장하지 않고 휘발성으로 조회후 버리는(?) 형태로 할것인지를 고민해야했다. 공모주 라는 데이터의 특성상 한번 정해진 메타 데이터가 상황에 따라 변경이 될수도 있다고 했기에(일정이 변경되거나 공모가가 변경되거나 등) DB에 저장을 하게 되면 이를 동기화(Sync)하는 비용이 추가로 생길것 같아서 알림을 보내기 직전에만 조회하고 버리는 형태를 생각했다.
　그렇게 데이터를 조회했다면 이를 입맛에 맞게 가공하고서 사용자에게 알림을 줘야한다. 알림을 발생시키는 방법은 매우 다양한데 뭔가 적은 비용으로 구성하고 싶었다. 즉, 알림을 받는 사용자가 10명, 100명, 1000명 이 되어도 (그렇게 될지는 모르겠지만;;) 내가 만든 서비스에서 알림수신인이 늘어나는 경우를 고려하지 않아도 되었으면 했다. 그에 생각한게 메신저 API. 그중에서도 텔레그램 API가 뭔가 이런 형식으로 딱일것 같았기 때문이다. 결국 데이터를 메세지 형태에 맞춰 한번만 발송하게 되면 1:N 형식(Broadcast)으로 텔레그램 채널을 구독하고 있는 사용자들에게 전송이 될테니 안성맞춤이었다.
　그럼 언제 어떤정보를 알려주는게 좋을까? 청약이 보통 오전에 시작하기 때문에 대략 매일 오전 9시에 관련 정보들을 보내주면 될 것 같았다. 3일 전에 청약을 시작하게 되니 미리 준비 하라는 알림. 그리고 청약 날짜가 도래해서 잊지 말고 청약을 신청 하라는 알림. 마지막으로 공모주가 상장을 하게 되는 알림. 이 세가지 알림만 잘 챙긴다면 필자같은 주린이들도 충분히 공모주 청약으로 치킨을 먹을수 있을꺼라 생각했다.
　마지막으로 이 모든 내용을 개발한 어플리케이션을 어느곳에 배포해야 하는지를 결정해야했다. 항상 머릿속에는 있었지만 한번도 안해본 클라우드 Paas 인 heroku가 딱일꺼라 생각했다. (실제로 해보지 않았던 지금까지는 그랬다&hellip;) 다들 토이프로젝트 서버로 잘 활용한다는 소리를 들었고 가장 큰 장점은 &lsquo;무료&rsquo;라는 점에서 heroku를 선택하지 않을수 없었다.
 참, 이번 토이프로젝트는 전부 &lsquo;무료 서비스&rsquo;를 이용하려 했다.]]></description></item><item><title>Elastic Stack으로 코로나19 대시보드 만들기 - 2부 : 대시보드</title><link>https://taetaetae.github.io/posts/make-dashboards-from-elasticstack-2/</link><pubDate>Wed, 17 Feb 2021 16:53:49 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/make-dashboards-from-elasticstack-2/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/make-dashboards-from-elasticstack-2/logo.jpg" referrerpolicy="no-referrer">
            </div>지난 포스팅에서는 ﻿데이터를 수급하며 전처리 과정을 거쳤고, Filebeat와 Logstash를 거쳐 Elasticsearch에 인덱싱 하는 것까지 알아보았다. 앞선 포스팅에서 이야기했지만 단순하게 데이터를 시각화 도구를 이용해서 대시보드를 만드는 게 아니라 데이터가 추가되면 만들어둔 대시보드에 자동으로 반영되는 흐름을 만들고 싶었다. 마침 파이프라인을 이틀 전에 만들었기 때문에 그동안의 빠진 데이터를 추가해야 하는 상황이다. 이 경우 Filebeat-Logstash-Elasticsearch 가 실행 중이라면 앞서 작성했던 파이썬 스크립트만 한번 실행해 주면 이틀 치 데이터가 파이프라인을 거쳐 Elasticsearch로 인덱싱이 된다. 즉, 별도로 데이터를 가져와서 재 가공하고 추가하는 다소 까다로운 작업이 미리 만들어둔 파이프라인 덕분에 한 번의 스크립트 실행으로 손쉽게 처리가 됨을 알 수 있다.
　이제는 쌓여있는 데이터를 가지고 시각화를 해볼 차례이다. ElasticStack에서는 Kibana라는 강력한 시각화 도구를 제공하는데 이번 포스팅에서는 Kibana를 이용해서 대시보드를 만드는 방법에 대해 알아보려 한다.
Visualize 　﻿Elasticsearch에 인덱싱 되어있는 데이터들은 기본으로 제공되는 REST API를 통해서 조회할 수 있고 JSON 형태로 결과가 나오기 때문에 이를 가지고 다양하게 시각화를 할 수도 있다. 하지만 Kibana에서는 데이터를 조회하고 UI로 표현하는 일련의 모든 행위를 클릭 몇 번으로 할 수 있게 해주기 때문에 전문가가 아니더라도 조금만 만져보면 누구나 만들 수 있다.
New Visualizaion!!" New Visualizaion!!  　버전업이 되면서 비쥬얼라이즈를 만드는 첫 화면 또한 변화가 생겼다. 기존에는 어떤 유형의 비쥬얼라이즈를 선택할 것인지에 대해 선택하는 화면부터 나왔는데 만드는 걸 보다 편리하게 도와주는 Lens, TSVB 같은 기능들이 먼저 반겨준다. 이 기능을 통해서 만드는 방법도 괜찮지만 보다 명시적으로 만들고 싶으니 하단에 Aggregation based을 선택해서 원하는 비쥬얼라이즈의 타입을 선택해 보자. 이후 생성되어 있는 인덱스를 선택하면 본격적으로 비쥬얼라이즈를 그릴 수 있는 화면이 나오는데 대시보드 화면 기준으로 만들어야 할 항목별로 살펴보자.
전체 수 ." .  　﻿확진자, 사망자, 격리 해제의 총합을 표현하려 한다. 이렇게 &lsquo;숫자&rsquo;를 표현하려 하는 경우 Metric을 활용하곤 한다. 우측에서 Aggregation 방법을 &lsquo;sum&rsquo;으로 설정하고 필드는 유형별로 각각 선택해 주자. 아래 &lsquo;Add&rsquo;버튼을 눌러 확진, 사망, 격리 해제 수를 모두 표시한 다음 저장을 눌러준다. Label을 지정하지 않으면 어떤 형태로 Aggregation을 했는지를 Label 영역에 보여주는데 그게 보기 싫다면 원하는 텍스트로 지정해 주는 것도 방법이다.
최근 수 ." .  　﻿확진자, 사망자, 격리 해제의 &lsquo;최근 데이터&rsquo;를 보여주는 게 목적이다. 이 경우 Aggregation을 Top Hit으로 선택하면 필드를 선택할 수 있게 되는데 하루의 데이터가 총 18 row이기 때문에 (서울, 부산, &hellip;, 제주, 검역) 18 row 을 전부 더한 값이 하루 기준의 합계가 된다. 여기서 정렬을 날짜 기준 내림차순으로 해줘야 가장 최근 데이터의 합계가 되는 점도 신경 써야 한다.
각 타입별 합계 ." .  　﻿지역별로 타입별 수를 보기 위해 Pie 타입으로 선택하여 진행한다. 타입별(예로 들어 확진이면 confirmed)로 합계를 구하기 위해 Aggregation을 &lsquo;sum&rsquo;으로 설정하면 빈 원이 나오지만 각 지역별로 차트를 잘라서 봐야 하기에 하단의 Buckets의 Add를 누르고 regieon의 필드를 Terms Aggregation 한다. 18 row의 데이터가 전부 보여야 하기에 정렬 개수를 늘리고 option 탭에서 보는 취향에 알맞게 설정값들을 바꿔준다.
타입별 추이 ." .  　﻿확진, 사망, 격리 해제 중에 사망을 제외하고 나머지 둘은 데이터의 크기가 크고 변화량이 비슷하기 때문에 x축은 시간으로 설정해두고 사망은 막대로, 나머지 둘은 라인으로 한 화면에서 표현하면 이 3가지 데이터를 한눈에 보기 좋을 것 같았다. Vertical bar 을 선택하고 x축(Buckets &gt; X-axis)은 데이터 타입인 convert_date로 설정한다. 다음으로 사망은 매일 몇 명 사망했는지 뚜렷하게 보기 위해 그냥 sum으로, 나머지 둘은 누적 합계가 더 의미 있어 보일 것 같아 Cumulative Sum으로 Aggregation을 한다.]]></description></item><item><title>Elastic Stack으로 코로나19 대시보드 만들기 - 1부 : 파이프라인 구성</title><link>https://taetaetae.github.io/posts/make-dashboards-from-elasticstack-1/</link><pubDate>Mon, 15 Feb 2021 17:50:12 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/make-dashboards-from-elasticstack-1/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/make-dashboards-from-elasticstack-1/logo.jpg" referrerpolicy="no-referrer">
            </div>﻿　얼마 전에 필자의 블로그를 보고 어느 교육 기관에서 ElasticStack에 대한 강의 요청이 들어왔다. 사실 관련 기술에 대해 지식이 아주 깊고 해박한 게 아니라서 약간의 반감부터 들었지만 ElasticStack을 전혀 모르는 사람들 기준으로 어떻게 돌아가는지에 대해서만 간단하게 소개하는 정도로 하면 된다고 하여 조심스럽지만 떨리는 마음으로 열심히 준비를 하기 시작했다. 그런데, 이런저런 이유로 갑자기 강의를 할 수 없게 되었고 그간 준비했던 내용들이 너무 아쉽지만 아무 소용이 없게 되어버렸다. 그냥 중단하기엔 아쉬운 마음이 너무 커서 준비했던 내용 중에 &lsquo;데이터를 가지고 대시보드를 만드는 부분&rsquo;은 누군가에겐 도움이 될까 싶어 블로그에 정리를 해보려 한다.
 ﻿강의를 준비한 올해 1월 중순엔 Elasticsearch 버전이 7.10.2이었는데 블로그를 쓰고 있는 지금은 벌써 7.11으로 버전 업 되었다. 내가 아는 오픈소스 중에 버전업이 가장 빠른데 그렇다고 기능이 확 바뀌거나 습득하기 어렵게 바뀌진 않았다. 그만큼 사용자가 무엇을 원하는지 명확히 알고 작은 단위로 조금씩 바뀌어 가는 모습이 꽤 인상적이다.
 　﻿작년 초부터 코로나19 바이러스가 전 세계적으로 퍼지기 시작했고 아직까지도 진행 중이다. 나도 전염되는 건 아닐까 하는 두려움에 어디에서 얼마나 발생했는지를 확인하기 어렵던 시절 우리나라의 뛰어난 개발자들은 누가 시키지도 않았는데 정말 감사하게도 그 현황을 한눈에 볼 수 있도록 여러 유형으로 코로나19 바이러스 대시보드를 만들기 시작한다. 그 덕분에 좀 더 현황을 보기에 편해졌고 더욱 조심하게 되는 계기가 되었다고 생각한다. 이제는 포털사이트나 각종 매체를 통해 손쉽게 코로나19 바이러스의 현황을 볼 수 있지만 이러한 데이터를 가지고 검색엔진이지만 대시보드를 구축하는데 훌륭한 기능을 가지고 있는 ElasticStack을 활용해서 &lsquo;나만의 대시보드&rsquo;를 만드는 걸 정리해보고자 한다. 본 포스팅의 일회성으로 데이터를 가지고 대시보드를 만드는 것에서 끝나는 게 아니라 지속적으로 데이터가 업데이트된다는 가정하에 전반적인 &ldquo;파이프라인&quot;을 구축한 뒤 대시보드를 만들어 두고 데이터만 갱신하면 자동으로 대시보드 또한 업데이트되는 것을 목적으로 한다. 전체 흐름" 전체 흐름 
 ﻿글을 모두 작성하고 보니 양이 생각보다 길어져서 데이터를 조회하고 필터링하여 Elasticsearch에 인덱싱 하는 대시보드를 만들기 위한 일종의 &ldquo;데이터 파이프라인&quot;을 구성하는 부분과 만들어진 데이터 기반으로 Kibana의 다양한 기능을 활용하여 대시보드를 만드는 2개의 포스팅으로 나누어 정리해보겠다.
 ﻿　최종적으로 만들게 될 대시보드의 모습은 다음과 같다. 최종 목표!" 최종 목표! 
대시보드 구성 준비 　﻿예전에는 Elasticsearch, Logstash, Kibana 3가지를 가지고 ELK라 불리다 Beat라는 경량 수집기 제품이 등장하며 이 모든 걸 ElasticStack라 부르기 시작했다. (공식 홈페이지 참고) 먼저 어떤 목표와 어떤 순서로 대시보드를 구성할 것인지에 대해 정리해봐야겠다.﻿
데이터 　﻿데이터는 공공데이터 포털에서 가져오려다 조회를 해보니 누락되는 날짜도 있었고 원하는 데이터의 품질이 생각보다 좋지 않아서 다른 곳을 찾아봐야 했다. 그러다 간결하게 정리한 데이터가 깃헙에 공개가 되어 있어서 그것을 사용하려 한다. 해당 데이터는 https://coronaboard.kr/ 에서도 사용되는 데이터라고 한다. ﻿
데이터 전처리(preprocessing) 　﻿원하는 데이터는 위 깃헙에서 제공하는 데이터 중에 지역별 발생 현황. 해당 데이터를 살펴보면 요일별로 데이터가 &lsquo;누적&rsquo;되어 저장되어 있다. 즉, 서울지역 기준으로 2020년 2월 17일에 14명이 발생했고 2020년 2월 18일에 한 명도 발생하지 않았는데 14명으로 &lsquo;누적&rsquo;되어 저장되어 있다. 사실 이대로 해도 큰 문제는 없지만 어디까지나 별도의 가공 없이 최대한 원본 데이터(raw) 가 있어야 데이터 분석 시 다양하게 활용이 가능하기에 데이터를 분석하기 전에 전처리 과정이 필요했다. 정리하면, 집계 수가 누적되지 않고 날짜 기준으로 집계된 수만 있는 데이터를 원했다.﻿
　﻿필자는 주로 java를 가지고 개발을 하지만 가끔 간단한 스크립트성 개발은 python을 활용하는 편이기에 다소 이쁜 코드는 아니지만 데이터를 조작하려 아래와 같은 코드를 작성하였다.
import csv, requests import pandas as pd CSV_URL = &#39;https://raw.githubusercontent.com/jooeungen/coronaboard_kr/master/kr_regional_daily.csv&#39; # 확진, 사망, 격리해제 yesterday_data = {} yesterday_data[&#39;서울&#39;] = [0, 0, 0] yesterday_data[&#39;부산&#39;] = [0, 0, 0] yesterday_data[&#39;대구&#39;] = [0, 0, 0] yesterday_data[&#39;인천&#39;] = [0, 0, 0] yesterday_data[&#39;광주&#39;] = [0, 0, 0] yesterday_data[&#39;대전&#39;] = [0, 0, 0] yesterday_data[&#39;울산&#39;] = [0, 0, 0] yesterday_data[&#39;세종&#39;] = [0, 0, 0] yesterday_data[&#39;경기&#39;] = [0, 0, 0] yesterday_data[&#39;강원&#39;] = [0, 0, 0] yesterday_data[&#39;충북&#39;] = [0, 0, 0] yesterday_data[&#39;충남&#39;] = [0, 0, 0] yesterday_data[&#39;전북&#39;] = [0, 0, 0] yesterday_data[&#39;전남&#39;] = [0, 0, 0] yesterday_data[&#39;경북&#39;] = [0, 0, 0] yesterday_data[&#39;경남&#39;] = [0, 0, 0] yesterday_data[&#39;제주&#39;] = [0, 0, 0] yesterday_data[&#39;검역&#39;] = [0, 0, 0] flag = False csv_data = [] with requests.]]></description></item><item><title>Swagger와 Spring Restdocs의 우아한 조합 (by OpenAPI Spec)</title><link>https://taetaetae.github.io/posts/a-combination-of-swagger-and-spring-restdocs/</link><pubDate>Tue, 22 Dec 2020 10:41:40 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/a-combination-of-swagger-and-spring-restdocs/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/a-combination-of-swagger-and-spring-restdocs/main.jpg" referrerpolicy="no-referrer">
            </div>﻿MSA 환경에서의 API 문서화는 어떤 식으로 구성하는 걸까? 예컨대, 모듈이 10개 있다고 하면 각 모듈마다 API 문서가 만들어질 테고 API 문서를 클라이언트에 제공하기 위해서 각각의 (10개의) URL를 전달해야 할 텐데 이게 과연 효율적일까? 물론 기능별로 URL이 분리된다는 장점이 있고 굳이 모아보자면 각 API 문서를 다시 한번 크롤링 하여 검색할 수 있도록 제공하는 것도 하나의 방법이 될 수 있다. 하지만 이러한 방법들은 요구 사항을 위한 별도의 작업을 하게 되니 일을 위한 일이 되는 것 같아 뭔가 아쉬웠다. 좋은 방법이 없을까?
고민의 시작 　한창 궁금증이 머릿속에서 지워지지 않았을때 Spring 한국 스프링 사용자 모임 페이스북 그룹에 문의도 해가며 방법을 찾아가고 있었다.
﻿닉네임이나 프로필 사진은 그들의 개인 정보를 위해 임의로 지정하였다." ﻿닉네임이나 프로필 사진은 그들의 개인 정보를 위해 임의로 지정하였다.  　﻿필자와 함께 개발자의 인생을 시작한 멋진 친구들에게 정확히 올해 6월 초에 고민을 털어놓으며 좋은 방법이 없을지에 대한 논의를 했던 적이 있다. 그런데 친구 중 한 명이 잊고 있었던 그 이슈에 대해서 다시 꺼내며 URL 하나를 던져준다. 참 고마운 친구들.
 Shout out 34. asuraiv, black9p
 언 반년이 지났으나 필자도 잊고 있었던 이슈를 그는 기억하고 있었다." 언 반년이 지났으나 필자도 잊고 있었던 이슈를 그는 기억하고 있었다.  　﻿올해 NHN FORWARD에서 진행했던 세션 중에서 MSA 환경에서 API 문서 관리하기: 생성부터 배포까지라는 제목의 내용이었고, 정확하게 필자가 고민했던 부분을 콕! 집어서 해결해 준 사례였다. 역시 세상엔 엄청난 고수들이 내가 고민했던 부분들을 이미(혹은 이후에라도) 고민하고 해결한 경우가 많다는 것을 느끼고 공유의 힘이 이렇게도 대단하구나 하며 놀라움을 금치 못하였다.﻿
  　﻿이번 포스팅에서는 OpenAPI Spec 을 활용하여 Spring Restdocs로 만들어지는 문서를 Swagger UI에서 보는 흐름을 실제로 구현해 보고자 한다. 즉, Swagger 나 Spring Restdocs 뭐로 만들든 간에 OpenAPI Spec에 맞춰서만 만든다면 한곳에서 볼 수 있겠다는 희망이 보였다. 며칠 전 작성한 OpenAPI 와 Swagger-ui 포스팅을 본 독자들은 지금의 포스팅을 작성하기 위한 밑거름이었다는 사실을 눈치챘을 수도 있을 것 같다.
 ﻿좋은 내용을 공유해 주신 (저의 고민을 완벽하게 해결해 주신) NHN FORWARD 발표자분께 이 포스팅을 빌어 감사의 인사를 보냅니다. :) 당장 팀 내에도 적용해봐야겠어요!!
 ﻿Spring Restdocs에서 OpenAPI Spec 추출 　﻿누가 또 친절하게 오픈소스로 만들어놨다. https://github.com/ePages-de/restdocs-api-spec 에서 관련 내용을 확인할 수가 있는데 해당 링크에서는 gradle 버전이고 https://github.com/BerkleyTechnologyServices/restdocs-spec 는 maven 버전이라고 한다. 마침 필자의 Github에 Maven 버전으로 SpringRestdocs를 세팅해둔 Repository 가 있어서 이를 활용해보고자 한다.﻿
pom.xml 추가 　﻿관련 dependency를 추가하자. jcenter라고 bintray.com 에서 운영되는 Maven Repository에 올려진 오픈소스이니 repository 도 추가해 주자.
&lt;properties&gt; &lt;restdocs-api-spec.version&gt;0.10.0&lt;/restdocs-api-spec.version&gt; &lt;restdocs-spec.version&gt;0.19&lt;/restdocs-spec.version&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jcenter&lt;/id&gt; &lt;url&gt;https://jcenter.bintray.com&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependency&gt; &lt;groupId&gt;com.epages&lt;/groupId&gt; &lt;artifactId&gt;restdocs-api-spec&lt;/artifactId&gt; &lt;version&gt;${restdocs-api-spec.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.epages&lt;/groupId&gt; &lt;artifactId&gt;restdocs-api-spec-mockmvc&lt;/artifactId&gt; &lt;version&gt;${restdocs-api-spec.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; ﻿위의 dependency에서 제공해 주는 모듈로 테스트의 SpringRestdocs를 만들었다면 OpenAPI Spec 을 만들어 주는 plugin 또한 추가해 주자
&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;jcenter&lt;/id&gt; &lt;url&gt;https://jcenter.bintray.com&lt;/url&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;plugin&gt; &lt;groupId&gt;com.github.berkleytechnologyservices.restdocs-spec&lt;/groupId&gt; &lt;artifactId&gt;restdocs-spec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${restdocs-spec.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;generate&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;specification&gt;OPENAPI_V3&lt;/specification&gt; &lt;format&gt;JSON&lt;/format&gt; &lt;outputDirectory&gt;${project.build.directory}/classes/static/docs&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; ﻿위 plugin 설정을 보면 format 을 JSON으로 한 것을 볼 수 있는데 YAML로도 만들 수 있다. 자세한 사용방법은 위에서 명시한 링크를 참고해보는 게 좋을 것 같다.
문서화 로직 추가 　﻿기존에 SpringRestdocs를 작성하는 로직은 org.springframework.restdocs.mockmvc.MockMvcRestDocumentation에서 제공해 주는 메서드를 사용했지만 위에서 이야기 한 오픈소스를 사용하기 위해 com.epages.restdocs.apispec.MockMvcRestDocumentationWrapper를 사용하도록 하자. 변경을 최소화하기 위해 import만 변경하도록 한다.
//import static org.springframework.restdocs.mockmvc.MockMvcRestDocumentation.*; import static com.epages.restdocs.apispec.MockMvcRestDocumentationWrapper.*; ﻿위와 같이 설정하고 Maven 빌드를 해보면 plugin에서 지정한 경로에 JSON 파일이 생성된 것을 확인할 수 있다.]]></description></item><item><title>OpenAPI 와 Swagger-ui 적용하기</title><link>https://taetaetae.github.io/posts/openapi-and-swagger-ui-in-spring-boot/</link><pubDate>Sun, 20 Dec 2020 11:42:40 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/openapi-and-swagger-ui-in-spring-boot/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/openapi-and-swagger-ui-in-spring-boot/main.png" referrerpolicy="no-referrer">
            </div>﻿API를 개발하고 사용방법에 대한 명세를 작성하는 방법은 여러 가지가 있다. 가장 심플하게 개발 코드와는 별도로 직접 수기로 작성하여 파일 혹은 문서 링크를 전달하는 방법이 있다. 하지만 개발 코드와 별도로 직접 작성을 한다는 점에서 오타/실수가 발생할 수 있고 최신화가 안되는 여러 가지 문제가 발생한다. 그에 등장한 API 문서화 자동화 툴의 양대 산맥인 SpringRestDocs 와 Swagger.
　﻿과거 SpringRestDocs 에 대한 포스팅을 했기에 이번엔 Swagger에 대한 사용방법에 대해 정리해보고자 한다. 이 둘의 장단점은 너무 뚜렷하기에 API문서를 제공하는 상황에 따라 적절하게 선택하여 사용할 수 있었으면 좋겠다.
﻿SpringBoot에 Swagger 적용 　기본 SpringBoot 가 셋팅되어 있다는 가정하에 Swagger 관련 dependency를 추가해주자. 아참, 이제부터의 프로젝트 셋팅은 Gradle로 하려한다. (물론 Maven으로 해도 무방하지만&hellip;)
dependencies { implementation &#34;io.springfox:springfox-boot-starter:3.0.0&#34; } 　﻿이후 JavaConfig 을 아래와 같이 설정하는데 아래 내용은 아주 기본 세팅이니 자세한 내용은 공식 도큐문서를 참고해 보면 좋을 것 같다. (물론 샘플 프로젝트를 만들며 필요할 것 같은 내용은 아래에서 설명하겠다.)﻿
@EnableSwagger2 @Configuration public class SwaggerConfig { @Bean public Docket api() { return new Docket(DocumentationType.SWAGGER_2) .select() .apis(RequestHandlerSelectors.any()) .paths(PathSelectors.any()) .build(); } } 　﻿테스트할 컨트롤러를 아래처럼 심플하게 작성하고(사칙연산&hellip;) 실행을 시킨 후 /swagger-ui/에 접속을 해보면 swagger 관련 javaConfig 하나만 추가했는데 문서가 만들어진 것을 확인할 수 있다. (http method는 편의상 다양하게 작성했으니 왜 DELETE 인가라는 의문은 접어두자.)
@RestController public class SampleController { @GetMapping(value = &#34;/addition&#34;) public Integer addition(Integer num1, Integer num2) { return num1 + num2; } @PostMapping(value = &#34;/subtraction&#34;) public Integer subtraction(Integer num1, Integer num2) { return num1 - num2; } @PutMapping(value = &#34;/multiplication&#34;) public Integer multiplication(Integer num1, Integer num2) { return num1 * num2; } @DeleteMapping(value = &#34;/division&#34;) public Integer division(Integer num1, Integer num2) { return num1 / num2; } } 기본 셋팅만 했는데 이런 화면이 나타났다." 기본 셋팅만 했는데 이런 화면이 나타났다.  　﻿위에서 했던 설정들 중 몇 가지만 좀 더 자세히 살펴보자.
   설정 설명     Docket ﻿Springfox 프레임 워크의 기본 인터페이스가 될 빌더로 구성을 위한 여러 가지 기본값과 편리한 방법을 제공하고 있다. 이후 select()로 ApiSelectorBuilder를 반환받아 사용할 수 있도록 해준다.   apis ﻿어떤 위치에 있는 API들을 가져올 것인가에 대한 정의. RequestHandlerSelectors.any()이라고 했으니 SpringBoot에서 기본으로 제공하는 basic-error-controller 도 API 문서로 만들어진 것을 확인할 수 있다. 특정 패키지만 적용하기 위해서는 RequestHandlerSelectors.basePackage(&quot;com.taetaetae.swagger.api&quot;) 와 같은 형식으로 지정하면 해당 패키지 하위에 있는 Controller를 기준으로 문서를 만들어 준다﻿.   paths ﻿이름에서도 눈치를 챌 수 있듯이 특정 path만 필터링해서 문서를 만들어 준다.   useDefaultResponseMessages ﻿기본 http 응답 코드를 사용해야 하는지를 나타내는 플래그    ﻿이외에도 security 나 공통으로 사용되는 파라미터 등 다양한 옵션을 설정할 수 있으니 가능하면 상황에 맞게 설정을 변경해 보는 것도 좋을 것 같다. 다른 설정들을 추가시켜서 좀 더 친절하게 만들어 보면 아래처럼 만들 수 있고 해당 코드는 Github에서 확인 가능하다.
API 문서화는 최대한 친절하게!!" API 문서화는 최대한 친절하게!!  OpenAPI 　﻿Swagger 공식 홈페이지를 이리저리 둘러보면 OpenAPI라는 내용이 많이 나온다. 그렇다면 OpenAPI는 무엇일까? 문서에 나와있는 내용을 직역해보면 Swagger 사양으로 알려져 있으며 RESTful 웹 서비스를 설명, 생성, 소비 및 시각화하기 위한 기계 판독 가능 인터페이스 파일에 대한 사양이라고 한다. 즉, API 자체를 설명하는 인터페이스 스펙이라고 이해를 해볼 수 있다. 위에서 만들어졌던 Swagger를 보면 http://localhost:8080/v2/api-docs?group=Test API 라고 나와있는데 이를 클릭해보면 아래와 같이 json 형태로 보인다.]]></description></item><item><title>Jenkins Job을 병렬로 실행해서 속도를 개선해보자. (by. Pipeline)</title><link>https://taetaetae.github.io/posts/jenkins-job-parallel-processing-by-pipeline/</link><pubDate>Sun, 06 Dec 2020 20:19:47 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/jenkins-job-parallel-processing-by-pipeline/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/jenkins-job-parallel-processing-by-pipeline/pipeline.jpg" referrerpolicy="no-referrer">
            </div>﻿　관리하는 URL이 200응답을 주고 있는지 모니터링을 한다고 가정해보자. 다양한 방법이 생각나겠지만 가장 처음으로 떠오른 건 단연 Jenkins. 간단하게 사용할 언어에 맞춰 Execute Script를 작성하고 스케줄링을 걸어 놓으면 큰 수고 없이 모니터링을 구성할 수 있게 된다. 아래는 python script로 작성해 보았다.
import requests url=&#34;http://모니터링url&#34; status_code = requests.get(url).status_code if status_code != 200: print(f&#39;응답 실패 :{url}, status : {status_code}&#39;) exit(1) ﻿　하지만 모니터링을 해야 하는 URL이 1개에서 여러 개로 늘어난다면 어떻게 될까? 단순하게 작성한 Script를 아래처럼 약간 수정하면 되긴 하지만 URL마다 응답속도가 다를 경우 순차적으로 실행하다 보니 실행 속도는 느릴 수밖에 없다.
import requests urls = [ &#34;http://모니터링url-1&#34;, &#34;http://모니터링url-2&#34;, &#34;http://모니터링url-3&#34; ] for url in urls: status_code = requests.get(url).status_code if status_code != 200: print(f&#39;응답 실패 :{url}, status : {status_code}&#39;) exit(1) ﻿ 이러한 경우, 빠른 속도를 보장하기 위해서는 병렬로 실행을 해야 한다는 건 누구나 다 알지만 그렇다고 Thread를 사용하기엔 벌써부터 덜컥 부담이 된다. 그렇다고 Job을 URL 개수만큼 늘리기에는 배보다 배꼽이 더 커버리고&hellip; 그러다 발견한 기능이 바로 Jenkins Pipeline!
　이번 포스팅에서는 Jenkins Job을 동시에 여러 번 사용해야 하는 경우를 Pipeline을 통해서 개선한 내용에 대하여 공유해보려 한다. Jenkins Pipeline에 대해 들어만 봤는데 이번에 실제로 사용해보니 생각보다 쉽게 개선할 수 있었고 옵션들을 상황에 맞게 조합을 잘 한다면 상당히 활용성이 높아 보이는 기능인 것 같다.
기존상황 　테﻿스트를 위해 임의로 느린 응답을 생성하도록 URL을 구성하고 위에서 이야기했던 것처럼 Job 하나에 아주 심플하게 Python script를 작성하고 실행해보도록 하자. 임의로 느린 응답은 http://slowwly.robertomurray.co.uk/ 에서 제공하는 기능을 활용하였다.﻿
import requests urls = [ &#34;http://slowwly.robertomurray.co.uk/delay/0/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/100/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/200/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/500/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/1000/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/2000/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/5000/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/10000/url/https://www.naver.com/&#34;, &#34;http://slowwly.robertomurray.co.uk/delay/20000/url/https://www.naver.com/&#34; ] for url in urls: status_code = requests.get(url).status_code if status_code != 200: print(f&#39;응답 실패 :{url}, status : {status_code}&#39;) exit(1) print(f&#39;응답성공 : {url}&#39;) 그래서 실행해보면 50초가 소요되었다. 자, 이제 개선을 해보자!
개선을 해보자 　﻿전체적인 개선의 흐름은 하나의 Job에 모니터링하고자 하는 url을 파라미터로 받아서 처리할 수 있도록 설정하고, 이를 Jenkins Pipeline 을 통해 여러 URL을 동시에 모니터링하게 구성하는 것이다. 그러면 두 개의 Job(파라미터로 받아 모니터링하는 Job, Jenkins Pipeline Job) 만으로 보다 빠르고 효율적인 구성을 할 수 있을 것으로 상상을 하고.
Job을 범용적으로 (Jenkins paramters 활용) 　﻿위에서 샘플로 작성하였던 Python script는 url 이 늘어날수록 Job 안에 script를 수정해야 한다. 그렇게 해도 무방하지만 이번 개선의 목표는 하나의 Job을 Pipeline 이 병렬로 컨트롤하도록 설정해야 했기 때문에 Jenkins Job에 파라미터를 받을 수 있도록 아래처럼 Jenkins Job 설정에 파라미터를 설정하고 Python script 또한 수정해 주자.
﻿Job &gt; 구성 &gt; 이 빌드는 매개변수가 있습니다" ﻿Job &gt; 구성 &gt; 이 빌드는 매개변수가 있습니다  import requests, os url = os.environ[&#39;url&#39;] status_code = requests.get(url).status_code if status_code != 200: print(f&#39;응답 실패 :{url}, status : {status_code}&#39;) exit(1) print(f&#39;응답성공 : {url}&#39;) 병렬 실행을 위한 Jenkins 설정 　﻿Jenkins Job 을 생성하면 기본적으로 Job마다의 대기열(Queue)이 있어 Job이 실행 중이라면 시작된 시간 순서대로 기다렸다가 앞선 Job이 종료가 되면 이어서 실행되는 구조이다. 하지만 우리는 Job을 병렬로 실행해야 했기에 Job 설정 중 필요한 경우 concurrent 빌드 실행 옵션을 켜줘서 기다리지 않고 병렬로 실행될 수 있도록 해준다.
﻿Job &gt; 구성 &gt; 필요한 경우 concurrent 빌드 실행" ﻿Job &gt; 구성 &gt; 필요한 경우 concurrent 빌드 실행  　﻿또한 Jenkins Job 자체는 병렬로 실행되도록 설정되었다 해도 기본적으로 Jenkins 자체의 대기열은 한정되어 있기 때문에 적당히 늘려줘서 여러 개의 Job이 대기 열 없이 동시에 실행될 수 있도록 해준다.]]></description></item><item><title>기술블로그 개편기 (by HUGO)</title><link>https://taetaetae.github.io/posts/blog-reorganization-by-hugo/</link><pubDate>Sun, 29 Nov 2020 18:12:15 +0900</pubDate><author>Author</author><guid>https://taetaetae.github.io/posts/blog-reorganization-by-hugo/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/blog-reorganization-by-hugo/hexo_to_hugo.png" referrerpolicy="no-referrer">
            </div>웹서비스 개발자라면 나만의 블로그쯤은 있어야지 하며 기술 블로그를 시작한 지도 어느덧 4년이 되었다. 처음엔 그저 새로 알게 된 기술이나 삽질하며 경험한 것들 중에 핵심만을 적어놓는 수준이었다. (지금 다시 보면 뭔가 오글거리는 건 기분 탓이겠지&hellip;) 그렇게 계속 글을 써오면서 글쓰기라는 것에 관심을 갖게 되고 내 글이 누군가에게 도움이 될 거라는 기대에 조금이라도 글을 잘 써보고자 단순 기록 용이 아닌 하나의 &lsquo;글&rsquo;을 쓰려고 노력해 온 것 같다.
　일주일에 한 개는 써야지. 한 달에 한 개는 써야지. 하며 자꾸 나 자신과의 타협을 하다가 최근에는 회사에서 운영하는 서비스 개편 때문에 정신없이 바쁘다는 핑계로 &lsquo;블로그&rsquo;에 &lsquo;ㅂ&rsquo;자도 생각하지 못하게 된다. 무엇이 문제일까?라는 생각은 결국 내 기술 블로그도 회사 서비스처럼 &lsquo;개편&rsquo;을 해보자는 생각으로 도달하게 되었고 간단할 것만 같았던 기술 블로그 개편 작업은 꽤 오랫동안 + 다양한 삽질들로 작업을 하게 된다.
　이번 포스팅에서는 기술 블로그를 개편하며 겪었던 내용들에 대해 정리해보고자 한다. 기존에 기술 블로그를 운영하시는 분들이나 이번에 새롭게 시작하시는 분들께 도움이 될 거라 기대한다. 더불어 서비스 &lsquo;출시&rsquo; 가 아닌 개편&rsquo;이라는 과정 속에서 느끼게 되었던 인사이트도 간략하게 작성해볼까 한다.
기술블로그 플랫폼 선택 　처음 블로그를 쓰기 시작했을 때 포털서비스의 글쓰기 플랫폼을 사용하지 않은 이유는 단 하나다. &lsquo;글쓰기&rsquo; 뿐만 아니라 개발자이기에 웹사이트(블로그)를 내 입맛에 맞게 커스터마이징 하기 위해서. 그 이유로 hexo 라는 프레임워크에 github의 호스팅을 사용하여 운영을 해왔다. 그렇게 블로그를 운영해오면서 느꼈던 불편했던 부분들과 개편을 하며 기대하는 부분들을 정리하면 아래와 같다.
 테마(UI)가 이뻐야 하고 기능들이 많으면 좋겠다. 기술 블로그인 만큼 코드가 많이 삽입되니 코드 표현 또한 이뻐야 한다. 테마 또는 프레임워크의 커뮤니티가 활발해야 한다. 페이지 생성 또는 만들어진 웹페이지의 성능이 좋아야 한다. 글을 작성하고 배포하는 과정이 심플하고 깔끔해야 한다. ﻿  　위와 같은 이유를 기반으로 검색을 해보다 SSG(쓱 쇼핑몰 아님, Static site generators)를 깔끔하게 정리해 놓은 사이트를 발견한다. 정말 다양한 플랫폼들을 살펴보며 필자에게 맞는 게 어떤 건지 고민하다 결국 hugo 를 선택하게 된다. hugo를 선택한 이유는 go라는 언어를 사용한다는 것과 (간접적으로라도 다른 언어를 경험해보고 싶어서 + go 언어가 빠르다는 소리를 어디선가 들어서) 테마들이 너무 다양했기 때문이다.
﻿아주 대놓고 빠르다고 하니&hellip; 쓰고 싶어진다." ﻿아주 대놓고 빠르다고 하니&hellip; 쓰고 싶어진다.  　결국 hugo에 hugo-ranking-trend라는 사이트에서 상위에 랭크가 되어있고 기술 블로그 성격에 적합할 것 같은 LoveIt이라는 테마를 사용하기로 결정하였다. 자 그럼 시작해볼까?!
hugo 는 어떻게 쓰는거야? ﻿　대부분의 오픈소스는 hello world 혹은 quick start 같이 처음 접하는 사람들을 위한 도큐먼트가 있기 마련. hugo도 마찬가지로 quick-start가 있었고 이를 천천히 따라 하면 생각보다 쉽게 초기 세팅을 할 수 있었&hellip; 을꺼라 기대했지만 약간 초기 설정 과정이 어려워서 남겨 두고자 한다.
 참고로 필자는 윈도 10 환경에서 구성하였다. mac이라면 더 쉽게 설정할 수 있는 것 같은데 이 부분은 OS의 차이에서 생겨나는 어쩔 수 없는 약간의 장벽이라 생각한다. 이쁜 테마와 새로운 환경을 사용할 수 있다는 기대감으로 꾹 참아본다.
 기본설정 　﻿git이 설치되어 있다는 가정하에 우선 hugo는 go 언어기반으로 돌아가기에 우선 go를 설치해야 한다. 다운로드페이지에서 환경에 맞는 설치 파일을 다운로드하고 설치를 해준다. 다음으로 패키지 관리자인 chocolatey 또한 설치가 필요하다. 공식 홈페이지페이지에서 나와있는 순서대로 진행하면 설치 완료. 필자는 여기서 진행이 잘 안됐었는데, &lsquo;관리자 권한&rsquo;으로 PowerShell 을 실행시켜야지만 성공을 할 수 있었다.﻿
﻿　위 설정이 완료되었으면 드디어 hugo를 설치해 주고 초기화를 해준 뒤 샘플로 글 하나를 만들고 서버를 띄우면 끝.
# chocolatey 에 의해 hugo 설치 choco install hugo -confirm # hugo 초기화 hugo new site quickstart # post 생성 hugo new posts/post-name.]]></description></item><item><title>빌드/테스트는 내가 해줄게. 너는 코딩에 집중해 (by GitHub Pull Request Builder)</title><link>https://taetaetae.github.io/2020/09/07/github-pullrequest-build/</link><pubDate>Mon, 07 Sep 2020 10:09:56 +0000</pubDate><author>Author</author><guid>https://taetaetae.github.io/2020/09/07/github-pullrequest-build/</guid><description><![CDATA[git 은 분산 버전 관리 시스템 중 가장 잘 알려져 있다고 해도 과언이 아닐 정도로 대부분의 시스템에서 사용되고 있는 것 같다. 이를 웹서비스에서 보다 편하게 사용할 수 있도록 한 시스템이 Github. Github 을 사용하는 이유 중에 가장 큰 이유를 하나만 이야기해보자면 바로 온라인상에서 코드 리뷰를 할 수 있는 pullRequest라는 기능 때문이 아닐까 조심스럽게 생각을 해본다.
　pullRequest는 work branch에서 작업한 내용을 base branch로 merge 전 꼭 코드 리뷰가 아니더라도 작업한 내용에 대해서 다양한 검사를 자동화할 수 있는 강력한 기능들이 많다. 이러한 자동화는 CI(지속적 통합) 관점에서 매우 중요한데 코드에 대해 체크해야 할 부분들(빌드, 테스트, 정적 분석 등)을 &ldquo;알아서&rdquo; 해준다면 작업자는 오롯이 비즈니스 로직 개발에 대해서만 신경 쓸 수 있으니 생산성 절약 측면에서 엄청난 효과를 볼 수 있다.
내가 하는일에만 집중할 수 있게! 출처 : https://www.clien.net/service/board/park/10453442" 내가 하는일에만 집중할 수 있게! 출처 : https://www.clien.net/service/board/park/10453442  이번 포스팅에서는 그중에서도 아주 간단한 설정만으로 work branch의 빌드 상태를 검사해 볼 수 있는 Jenkins의 Github Pull Request Builder를 설치 및 활용해 보고자 한다.
 사실 최근 팀에서 CI 서버를 이전해야 했었다. 머릿속에서는 어떻게 하면 되겠지 싶었지만 막상 해보려니 Jenkins 버전업도 되었고 뭐부터 해야 할지 허둥대는 필자가 부끄러웠다. 이참에 정리를 해보며 다시 한번 리마인드 하는 시간을 가져보고자 한다. (이래서 기억보다 기록이 중요하다.)
 준비물 　전체적인 흐름은 아래 그림처럼 흘러가기 때문에 당연히 서버에 Jenkins 가 설치되어 있어야 한다. Jenkins 설치는 필자의 포스팅(Jenkins 설치 치트키)를 참고해 보는 것도 좋을 것 같다.
전체적인 흐름" 전체적인 흐름  　참고로 필자는 GitHub Enterprise 버전에서 사용했는데 일반 Github에서도 동일한 방법으로 사용 가능하다.
Github과 Jenkins의 연동을 위한 2가지 설정 　Github 과 Jenkins 가 통신이 되도록 설정해 줘야 한다. 그래야 Github의 코드를 받아서 Jenkins 가 빌드를 하고 그 빌드 결과를 다시 Github에 리포트가 가능해지기 때문이다. 먼저 첫 번째로 ssh 설정으로 Github의 코드를 가져오도록 ssh 설정을 해두자. ssh 설정하는 방법은 필자의 포스팅(Github과 Jenkins 연동하기)편을 확인해보면 될 것 같다.
　그다음으로 아래에서 이야기할 GitHub Pull Request Builder라는 Jenkins plugin 이 빌드가 끝난 뒤에 결과를 리포팅 해줄 수 있는 인증 토큰을 발급받아두자. Github &gt; Settings &gt; Developer settings &gt; Personal access tokens 화면에서 키를 생성하고 만들어진 키를 저장해 둔다. (이 키는 보안에 유의해야 하고, 화면 경고(?)에서도 볼 수 있듯이 키는 생성 시 한 번밖에 볼 수 없기 때문에 미리 저장해 둬야 한다.)
인증토큰을 미리 받아두자." 인증토큰을 미리 받아두자.  Jenkins 설정 　Jenkins &gt; 관리 &gt; pluginManager에 들어가 GitHub Pull Request Builder를 검색 후 설치해 준다. 그러고 나서 Jenkins &gt; 관리 &gt; 환경설정에 들어가 보면 아래와 같이 GitHub Pull Request Builder 항목이 생긴 것을 확인할 수 있고 위에서 설정한 인증토큰을 아래처럼 등록 후 저장을 한다.
credentials 을 위에서 발급받은 인증토큰으로 등록해준다." credentials 을 위에서 발급받은 인증토큰으로 등록해준다.  　Jenkins job을 하나 만들고 pullRequest 가 발생했을 때 자동으로 실행될 수 있도록 설정을 해준다. 먼저 General 탭에 Github project에 Github url 을 적어주고
 　소스 코드 관리 탭에서 ssh 주소를 적고 위에서 미리 설정한 ssh 키로 credentials 값을 넣어준다. 전에도 이야기했지만 이 부분에서 오류가 발생하면 빨간색 글씨로 오류 내용이 나오고 아래 화면처럼 오류가 없다면 아무것도 안 나온다. Refspec 에 +refs/pull/*:refs/remotes/origin/pr/* 라고 적어주고 브랜치 설정은 파라미터로 받아와서 pullRequest를 발생시킨 브랜치를 빌드 할 수 있도록 ${sha1} 라고 적어주자.]]></description></item><item><title>스프링 부트에 필터를 '조심해서' 사용하는 두 가지 방법</title><link>https://taetaetae.github.io/2020/04/06/spring-boot-filter/</link><pubDate>Mon, 06 Apr 2020 23:59:36 +0000</pubDate><author>Author</author><guid>https://taetaetae.github.io/2020/04/06/spring-boot-filter/</guid><description><![CDATA[웹 어플리케이션에서 필터를 사용하면 중복으로 처리되는 내용을 한곳에서 처리할 수 있다거나 서비스의 다양한 니즈를 충족시키기에 안성맞춤인 장치인것 같다. 필터란 무엇인가 에 대한 내용은 워낙에 다른 블로그나 공식 도큐먼트에서 자세하게 그리고 다양하게 설명하고 있기에 기본 개념에 대해서는 설명하지 않도록 하려 한다. 이번 포스팅에서는 스프링 부트를 사용하면서 어노테이션이라는 간편함에 취해(?) &ldquo;돌격 앞으로, 닥공&rdquo; 의 자세로 개발을 하려했던 필자를 보고 &ldquo;반성&quot;의 자세로 필터를 등록하는 방법에 대해 명확하게 정리를 하고자 한다. 마지막으로는 아주 간단하면서도 엄청나게 위험한 필터 설정 사례에 대해서도 짚고 넘어가보자. 그냥 넘어가면 아쉬우니, 한번이라도 &lsquo;spring&rsquo; 이라는 framework 를 접해본 사람이라면 봤을법한 그림을 첨부하는것으로 필터란 무엇인가 에 대한 설명을 대신하는게 좋겠다.
출처 : https://justforchangesake.wordpress.com/2014/05/07/spring-mvc-request-life-cycle/" 출처 : https://justforchangesake.wordpress.com/2014/05/07/spring-mvc-request-life-cycle/  방법을 설명하기 전에 동일하게 사용될 필터와 컨트롤러 코드를 보면 다음과 같다.
 필터  @Slf4j public class MyFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { log.info(&#34;init MyFilter&#34;); } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { log.info(&#34;doFilter MyFilter, uri : {}&#34;, ((HttpServletRequest)servletRequest).getRequestURI()); filterChain.doFilter(servletRequest, servletResponse); } @Override public void destroy() { log.info(&#34;destroy MyFilter&#34;); } }  테스트 할 컨트롤러  @Slf4j @RestController public class SampleController { @GetMapping(&#34;/test&#34;) public String test() { return &#34;test&#34;; } @GetMapping(&#34;/filtered/test&#34;) public String filteredTest() { return &#34;filtered&#34;; } } 방법 1 : FilterRegistrationBean 아주 간단하게, 일반 url 하나와 필터에 적용할 url 두개를 만들고 설정하려 한다. FilterRegistrationBean 을 이용해서 위에서 만들었던 필터를 아래처럼 등록해보자.
@SpringBootApplication public class Method1Application { public static void main(String[] args) { SpringApplication.run(Method1Application.class, args); } @Bean public FilterRegistrationBean setFilterRegistration() { FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new MyFilter()); // filterRegistrationBean.setUrlPatterns(Collections.singletonList(&#34;/filtered/*&#34;)); // list 를 받는 메소드 	filterRegistrationBean.addUrlPatterns(&#34;/filtered/*&#34;); // string 여러개를 가변인자로 받는 메소드 	return filterRegistrationBean; } } 위 주석에도 적었지만 filterRegistrationBean 의 &ldquo;setUrlPatterns&rdquo; 와 &ldquo;addUrlPatterns&rdquo; 의 차이는 별거 없다. list 자체를 받을건지 아니면 가변인자로 계속 추가 할것인지. 이렇게 되면 &ldquo;/filtered/&ldquo;으로 &ldquo;시작&quot;하는 패턴의 url의 요청이 오게 되면 등록한 필터를 통과하게 된다.
 실행 : 필터 생성  /\\ / ___&#39;_ __ _ _(_)_ __ __ _ \ \ \ \ ( ( )\___ | &#39;_ | &#39;_| | &#39;_ \/ _` | \ \ \ \  \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &#39; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.6.RELEASE) 2020-04-06 23:45:01.225 INFO 14672 --- [ main] c.t.s.method1.Method1Application : No active profile set, falling back to default profiles: default 2020-04-06 23:45:02.153 INFO 14672 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http) 2020-04-06 23:45:02.168 INFO 14672 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat] 2020-04-06 23:45:02.168 INFO 14672 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.33] 2020-04-06 23:45:02.361 INFO 14672 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2020-04-06 23:45:02.362 DEBUG 14672 --- [ main] o.s.web.context.ContextLoader : Published root WebApplicationContext as ServletContext attribute with name [org.springframework.web.context.WebApplicationContext.ROOT] 2020-04-06 23:45:02.362 INFO 14672 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 1082 ms 2020-04-06 23:45:02.391 DEBUG 14672 --- [ main] o.s.b.w.s.ServletContextInitializerBeans : Mapping filters: filterRegistrationBean urls=[/filtered/*] order=2147483647, characterEncodingFilter urls=[/*] order=-2147483648, formContentFilter urls=[/*] order=-9900, requestContextFilter urls=[/*] order=-105 2020-04-06 23:45:02.391 DEBUG 14672 --- [ main] o.]]></description></item><item><title>조금 더 괜찮은 Rest Template 2부 - Circuit-breaker</title><link>https://taetaetae.github.io/2020/03/29/better-rest-template-2-netflix-hystrix/</link><pubDate>Sun, 29 Mar 2020 23:09:16 +0000</pubDate><author>Author</author><guid>https://taetaetae.github.io/2020/03/29/better-rest-template-2-netflix-hystrix/</guid><description><![CDATA[<div class="featured-image">
                <img src="/images/better-rest-template-2-netflix-hystrix/netflix_hystrix.jpg" referrerpolicy="no-referrer">
            </div>지난 포스팅에서는 Retryable 를 활용해서 간헐적인 네트워크 오류를 &ldquo;재시도&quot;를 함으로써 아주 간단하면서도 강력하게 해결할 수 있는 방법에 대해 알아보았다. 실제로 필자가 운영하는 서비스 에서도 Retryable 를 이용하기 전과 후를 비교해보면 간헐적인 네트워크 오류의 빈도수가 확실히 줄어든것을 확인할 수 있었다. 이렇게 &ldquo;재시도&quot;를 해서 요청했을때 성공 응답을 받을 경우엔 문제가 안되지만 네트워크 오류가 아닌 실제로 호출을 받는 해당 서버에서 문제가 발생했다면 어떨까? 예컨대, 해당 서버에서 DB를 조회하는 API를 호출한다고 가정했을때 DB 자체에서 어떠한 오류가 난다면. 이런 경우는 단순히 &ldquo;재시도&quot;로 해결할 수 없는 문제다.
물론 Retryable 의 Recover 어노테이션을 활용했기 때문에 클라이언트 즉, 사용자에게는 오류응답이 발생을 안했겠지만 호출 받는 서버 자체에서의 에러가 발생하는데 이런식의 재시도를 계속 시도한다면 호출 받는 서버 입장에서는 이 &ldquo;재시도&rdquo; request 또한 &ldquo;부하&rdquo; 로 받게 되고 결국 2차, 3차 장애가 이어질 수 밖에 없다.
기존 한덩어리로 관리되던 Monolithic Architecture 에서는 자체적으로 관리하기 때문에 이러한 에러 컨트롤 또한 자체적으로 관리를 할 수 있지만, 모듈이 모듈을 호출하게 되는 Microservice Architecture 로 바뀌다보니 이런 &ldquo;연쇄 장애(?)&rdquo; 같은 현상이 발생하게 되는 경우가 있다. 호출을 받는 서버의 상태가 이상하면 (에러응답이 지정한 임계치를 벗어나는 수준으로 맞춰서 발생한다면) 적절하게 호출을 하지 않고 (2차 장애를 내지 않도록 호출 자체를 하지 않고) 어느정도 기다리다 클라이언트에게는 에러응답이 아닌 미리 정해둔 응답을 내려주고, 에러가 복구되면 다시 호출하도록 하는 &ldquo;무언가&rdquo; 가 필요하지 않을까?
연쇄 장애. 제발 멈춰&hellip; 출처 : http://dpg.danawa.com/mobile/community/view?boardSeq=175&amp;listSeq=4066389" 연쇄 장애. 제발 멈춰&hellip; 출처 : http://dpg.danawa.com/mobile/community/view?boardSeq=175&amp;listSeq=4066389  지난 포스팅에 이어 이번 포스팅 에서는 그 &ldquo;무언가&rdquo;. 즉, Circuit-breaker 에 대해 알아보고 직접 구현 및 테스트 하면서 돌아가는 원리에 대해 이해 해보고자 한다. 막상 개념은 머릿속에 있지만 직접 구현해보지 않으면 내것이 아니기에, 직접 구현하고 설정값들을 바꿔가면서 언젠가 필요한 순간에 꺼내서 사용할 수 있는 나만의 &ldquo;무기&rdquo; 를 만들어 보고자 한다.
Circuit breaker ? (한국 발음으로) 서킷브레이커를 검색해보면 주식시장 관련된 내용이 꽤 나온다. (앗, 잠깐 눈물좀&hellip;) 서킷 브레이커. 이 용어는 다양한 곳에서 사용되는데 &ldquo;회로 차단기&rdquo; 라고도 검색이 된다. 해당 내용을 발췌해보면 다음과 같다.
 회로 차단기는 전기 회로에서 과부하가 걸리거나 단락으로 인한 피해를 막기 위해 자동으로 회로를 정지시키는 장치이다. 과부하 차단기와 누전 차단기로 나뉜다. 퓨즈와 다른 점은, 차단기는 어느 정도 시간이 지난 뒤, 원래의 기능이 동작하도록 복귀된다.
 여기서 가장 중요한 문장은 &ldquo;피해를 막기 위해 자동으로 회로를 정지시키는&rdquo;, &ldquo;어느정도 시간이 지난뒤 원래의 기능이 동작하도록 복귀된다&rdquo; 이 부분이 가장 중요한 것 같다. 시스템 구성이 점점 Microservice Architecture 로 바뀌어 가는 시점에서 이러한 &ldquo;서킷브레이커&quot;는 자동으로 모듈간의 호출 에러를 감지하고 위에서 말한 &ldquo;연쇄 장애&quot;를 사전에 막을 수 있는 아주 중요한 기능이라 생각된다.
&ldquo;circuit breaker spring&rdquo; 이라는 키워드로 검색해보면 이러한 고민을 이미 Netflix 라는 회사에서 Hystrix 라는 이름으로 개발이 된것을 알 수 있다. 이 core 모듈을 Spring 에서 한번 더 감싸서 Spring Boot 에서 사용하기 좋게 spring-cloud-starter-netflix-hystrix 라는 이름으로 만들어 둔 것이 있는데 이것을 활용해 보기로 하자.
구현 늘 그랬듯이 SpringBoot 프로젝트를 만들고 테스트할 Controller 를 만들어 주자. 원래대로라면 호출을 하는 모듈과 호출을 받는 모듈, 2개의 모듈을 만들어서 테스트 해야 하지만 편의를 위해 하나의 모듈에서 두개의 Controller 을 만들고 테스트 해보는 것으로 하자.
@RestController public class MainController { private final MainService mainService; @GetMapping(&#34;index&#34;) public String index(String key){ return mainService.getResult(key); } public MainController(MainService mainService) { this.mainService = mainService; } } @Slf4j @Service public class MainService { private RestTemplate restTemplate; public String getResult(String key) { return restTemplate.]]></description></item></channel></rss>